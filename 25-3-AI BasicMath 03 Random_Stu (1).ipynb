{"cells":[{"cell_type":"markdown","source":["\n","-----\n","\n","### Numpy Random\n","\n","**수업 목표:** Numpy의 Random 함수를 활용하여 불확실성을 표현하고, 이를 통해 인공지능의 기본 원리를 체험한다.\n","\n","**수업 흐름:**\n","\n","  * **(10분) 도입:** 인공지능은 왜 '무작위'를 필요로 할까? (가중치 초기화, 데이터 증강 등)\n","  * **(80분) 예제 풀이 (1번 \\~ 10번):** 문제 제시 -\\> 학생 스스로 코드 작성 -\\> 함께 코드 리뷰 및 AI 개념 학습\n","  * **(10분) 마무리:** 오늘 배운 내용 정리 및 인공지능에서 Random의 중요성 강조\n","\n","-----\n","\n","### \\#\\# 1. [가중치 초기화] 신경망의 첫걸음\n","\n","**문제 제시:**\n","인공지능 신경망의 학습 시작 전, 각 연결의 중요도(가중치)를 무작위로 설정해야 합니다. 2개의 입력을 받아 3개의 출력으로 나가는 아주 작은 신경망 층의 가중치 행렬(2x3)을 `-1`에서 `1` 사이의 무작위 실수로 생성해 보세요.\n","\n","**파이썬 코드:**"],"metadata":{"id":"Z0XYfwTRIWZQ"}},{"cell_type":"code","source":["import numpy as np\n","\n","# 2x3 크기의 가중치 행렬을 -1과 1 사이의 균일한 분포에서 무작위로 생성\n","# np.random.rand()활용\n","weights1 =                    #여기\n","\n","# np.uniform()사용\n","weights2 =                    #여기\n","\n","print(\"초기화된 가중치 행렬1:\")\n","print(weights)\n","\n","print(\"초기화된 가중치 행렬2:\")\n","print(weights2)"],"outputs":[],"execution_count":null,"metadata":{"id":"pwFTyhUfIWZT"}},{"cell_type":"code","source":[],"metadata":{"id":"PZyvtBss3Xqj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**관련 인공지능 개념:**\n","**가중치 초기화 (Weight Initialization):** 신경망이 학습을 시작할 때, 모든 가중치가 동일하면 올바른 방향으로 학습하기 어렵습니다. 그래서 **무작위 값으로 작게 초기화**하여 각 연결이 서로 다른 출발점에서 학습을 시작하도록 합니다. 이는 모델이 더 빠르고 효과적으로 정답을 찾아가도록 돕는 중요한 첫 단계입니다.\n","\n","\n","요약하자면, 신경망의 가중치를 **서로 다른 작은 무작위 값**으로 설정하는 것은 마치 여러 등산가들이 산의 **서로 다른 위치에서 출발**하게 만드는 것과 같습니다.\n","\n"],"metadata":{"id":"4Zv_2N7LIWZU"}},{"cell_type":"markdown","source":["\n","-----\n","\n","### \\#\\# 2. [데이터 증강] 훈련 데이터 부풀리기\n","\n","**문제 제시:**\n","가지고 있는 이미지 데이터가 부족할 때, 원본 이미지를 약간씩 변형하여 훈련 데이터의 양을 늘릴 수 있습니다. 10x10 크기의 픽셀 값을 가진 가상의 이미지에 \\*\\*무작위 노이즈(잡음)\\*\\*를 추가하여 새로운 이미지를 만들어 보세요. (노이즈는 평균이 0이고 표준편차가 0.1인 정규분포를 따르게 합니다.)\n","\n","**파이썬 코드:**"],"metadata":{"id":"rzfp_Tjm9t-3"}},{"cell_type":"code","source":["import numpy as np\n","\n","# 0과 1 사이의 값으로 채워진 10x10 가상 이미지 생성\n","original_image = np.random.rand(10, 10)\n","\n","# 이미지와 같은 크기의 무작위 노이즈 생성 (정규분포)\n","noise = np.random.normal(              )  # 여기\n","\n","# 원본 이미지에 노이즈를 더해 새로운 이미지 생성\n","augmented_image = original_image + noise\n","\n","# 픽셀 값이 0~1 범위를 벗어나지 않도록 조정\n","augmented_image = np.clip(                  ) # 여기\n","\n","print(\"노이즈가 추가된 이미지 (일부):\")\n","print(augmented_image[:3, :3])"],"outputs":[],"execution_count":null,"metadata":{"id":"EN5cgwJ3IWZU"}},{"cell_type":"code","source":[],"metadata":{"id":"cO_GuG343YzN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**관련 인공지능 개념:**\n","**데이터 증강 (Data Augmentation):** 딥러닝 모델의 성능은 데이터의 양과 질에 크게 좌우됩니다. 데이터 증강은 기존 데이터를 **회전, 확대, 노이즈 추가** 등 다양한 방법으로 변형하여 데이터의 수를 인위적으로 늘리는 기술입니다. 이를 통해 모델이 다양한 상황에 대처하는 능력을 길러 \\*\\*과적합(Overfitting)\\*\\*을 방지하고 일반화 성능을 높일 수 있습니다.\n","\n"],"metadata":{"id":"4-CO2LJpIWZU"}},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# 3. [드롭아웃] 똑똑한 신경망 만들기\n","\n","**문제 제시:**\n","신경망 학습 시, 일부러 몇 개의 노드(뉴런)를 무작위로 꺼서 학습을 방해하는 '드롭아웃' 기법이 있습니다. 10개의 노드가 있는 신경망 층에서 **20%의 노드를 무작위로 선택하여 비활성화**시켜 보세요. (1은 활성화, 0은 비활성화)\n","\n","**파이썬 코드:**"],"metadata":{"id":"W_ztxDfi_RBz"}},{"cell_type":"code","source":["import numpy as np\n","\n","p_dropout = 0.2  # 드롭아웃 비율\n","# 20%라는 건 무작위성에 의해 1개,3개가 나올 수 있지만 평균적으로 10개 중에서 20%라는 의미\n","num_nodes = 10   # 노드의 개수\n","\n","# 0과 1 사이의 무작위 수를 10개 생성\n","random_values = np.random.rand(num_nodes)\n","print(random_values)\n","\n","# 드롭아웃 마스크 생성: 무작위 수가 드롭아웃 비율보다 크면 1(활성), 작으면 0(비활성)\n","dropout_mask = (                      ).astype(int)  # 여기\n","\n","print(f\"드롭아웃 비율: {p_dropout}\")\n","print(f\"생성된 드롭아웃 마스크: {dropout_mask}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"nqzASQuMIWZU"}},{"cell_type":"code","source":[],"metadata":{"id":"4wvYlWEH3ZdJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**관련 인공지능 개념:**\n","**드롭아웃 (Dropout):** 신경망이 특정 뉴런에만 너무 의존하여 학습하는 것을 막기 위한 규제(Regularization) 기법입니다. 훈련 과정에서 **일부 뉴런을 무작위로 무시**함으로써, 신경망이 여러 뉴런을 균형 있게 활용하도록 유도합니다. 이는 마치 팀 프로젝트에서 몇 명이 결석해도 일이 진행되도록 만드는 것과 같으며, 모델의 **일반화 성능을 크게 향상**시킵니다.\n","\n","드롭아웃은 **팀 프로젝트에서 일부러 몇 명을 무작위로 결석시키는 것**과 같습니다. 🤯\n","\n"],"metadata":{"id":"DDoH3nv0IWZV"}},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# 4. [미니배치-mini batch] 훈련 데이터 나누기\n","\n","**문제 제시:**\n","전체 훈련 데이터를 한 번에 학습시키는 것은 비효율적입니다. 그래서 데이터를 작은 묶음(미니배치)으로 나누어 학습합니다. 1000개의 데이터가 있을 때, 이 중에서 **중복을 허용하지 않고 무작위로 32개의 데이터를 뽑아** 하나의 미니배치를 구성해 보세요.\n","\n","**파이썬 코드:**"],"metadata":{"id":"UperGLiy_wWu"}},{"cell_type":"code","source":["import numpy as np\n","\n","total_data_size = 1000\n","batch_size = 32\n","\n","# 0부터 999까지의 인덱스 생성\n","data_indices = np.arange(total_data_size)\n","\n","# 인덱스 중에서 32개를 무작위로, 비복원(중복X) 추출하여 미니배치 구성\n","mini_batch_indices = np.random.choice(           , size=        , replace=         )  #여기\n","\n","print(f\"전체 데이터 인덱스 크기: {total_data_size}\")\n","print(f\"미니배치 크기: {batch_size}\")\n","print(f\"선택된 미니배치 인덱스 (일부): {mini_batch_indices[:10]}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"AAbKw27dIWZV"}},{"cell_type":"code","source":[],"metadata":{"id":"qV7olEQH3aQQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**관련 인공지능 개념:**\n","**미니배치 경사 하강법 (Mini-batch Gradient Descent):** 전체 데이터를 사용하는 것(배치)과 데이터 1개씩 사용하는 것(확률적)의 장점을 절충한 방식입니다. 데이터를 **적당한 크기의 묶음(미니배치)으로 나누어** 학습을 진행합니다. 이를 통해 안정적이면서도 빠른 속도로 모델을 학습시킬 수 있어, 대부분의 딥러닝 모델에서 표준적으로 사용되는 학습 방법입니다.\n","\n"],"metadata":{"id":"067C1dwGIWZV"}},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# 5. [강화학습] 행동 선택하기\n","\n","**문제 제시:**\n","강화학습에서 에이전트(로봇)는 현재 상태에서 어떤 행동을 할지 결정해야 합니다. '탐험'을 위해 가끔은 **무작위 행동**을 해야 합니다. 4가지 행동(상, 하, 좌, 우)이 있을 때, 10%의 확률(엡실론)로 무작위 행동을, 90%의 확률로 가장 좋다고 알려진 행동('우'로 이동)을 선택하는 로직을 구현해 보세요.\n","\n","**파이썬 코드:**"],"metadata":{"id":"zrdeQs92DUVN"}},{"cell_type":"code","source":["import numpy as np\n","\n","epsilon = 0.1  # 탐험 확률 (무작위 행동을 할 확률)\n","actions = ['상', '하', '좌', '우']\n","best_action = '우'\n","selected_action = np.array([])\n","result=[]\n","i=0\n","while(i<30):\n","  # 0과 1 사이의 무작위 수 생성\n","  if np.random.rand() < epsilon:\n","    # 탐험: 무작위 행동 선택\n","    selected_action = np.append(             ,np.random.choice(          ))  #여기\n","    result.append('탐험')\n","  else:\n","    # 활용: 가장 좋은 행동 선택\n","    selected_action = np.append(              ,             ) #여기\n","    result.append('활용')\n","  i+=1\n","\n","print(f\"선택된 행동: {selected_action}\")\n","print(f\"결과:{result}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"ezcBJYHgIWZW"}},{"cell_type":"code","source":[],"metadata":{"id":"CP8UyPMo3bJy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**관련 인공지능 개념:**\n","\n","**엡실론-그리디 (Epsilon-Greedy) 정책:** 강화학습에서 \\*\\*탐험(Exploration)\\*\\*과 **활용(Exploitation)** 사이의 균형을 맞추는 대표적인 전략입니다. 대부분의 경우(1-epsilon 확률) 지금까지의 경험으로 가장 좋다고 알려진 행동(활용)을 하지만, 가끔은(epsilon 확률) 새로운 가능성을 찾기 위해 무작위 행동(탐험)을 합니다. 이를 통해 더 나은 보상을 얻을 수 있는 새로운 방법을 끊임없이 탐색합니다."],"metadata":{"id":"zLyjqB_gIWZW"}},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# 6. [데이터 샘플링] 클래스 불균형 맞추기\n","\n","**문제 제시:**\n","스팸 메일 데이터가 990개, 정상 메일 데이터가 10개로 심각한 불균형 상태입니다. 이대로 학습하면 모델은 모든 메일을 스팸으로 예측할 가능성이 높습니다. 정상 메일 10개에 맞춰 **스팸 메일 990개 중에서 무작위로 10개만 뽑아** 데이터셋의 균형을 맞춰보세요.\n","\n","**파이썬 코드:**"],"metadata":{"id":"7r2euEv3ZMVv"}},{"cell_type":"code","source":["import numpy as np\n","\n","# 0~989: 스팸 메일 인덱스, 990~999: 정상 메일 인덱스\n","num_spam = 990\n","num_ham = 10\n","\n","spam_indices = np.arange(          ) #여기\n","\n","# 스팸 메일 인덱스 중에서 정상 메일 수만큼 무작위로 샘플링 (언더샘플링)\n","undersampled_spam_indices = np.random.choice(                  , size=      , replace=         )  #여기\n","\n","print(f\"정상 메일 수: {num_ham}\")\n","print(f\"언더샘플링된 스팸 메일 인덱스:\")\n","print(undersampled_spam_indices)"],"outputs":[],"execution_count":null,"metadata":{"id":"p4JcJInsIWZW"}},{"cell_type":"code","source":[],"metadata":{"id":"bFL1w1Kn3buP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**관련 인공지능 개념:**\n","**샘플링 (Sampling):** 데이터의 클래스(범주) 비율이 심하게 차이 나는 **불균형 데이터** 문제를 해결하기 위한 기법입니다. 수가 많은 클래스의 데이터를 무작위로 줄이는 **언더샘플링**과, 수가 적은 클래스의 데이터를 복제하거나 새로 생성하여 늘리는 **오버샘플링**이 있습니다. 이를 통해 모델이 소수 클래스의 특징도 충분히 학습하도록 돕습니다.\n","\n","-----\n","\n","### \\#\\# 7. [하이퍼파라미터 탐색] 최적의 값 찾기\n","\n","**문제 제시:**\n","모델의 성능을 결정하는 중요한 값들(학습률, 은닉층 크기 등)을 '하이퍼파라미터'라고 합니다. 최적의 조합을 찾기 위해 \\*\\*무작위 탐색(Random Search)\\*\\*을 수행합니다. 학습률은 `0.001`에서 `0.1` 사이, 은닉층의 크기는 `32`에서 `512` 사이의 정수 값 중 5개의 조합을 무작위로 뽑아보세요.\n","\n","**파이썬 코드:**"],"metadata":{"id":"5-478F-qIWZW"}},{"cell_type":"code","source":["import numpy as np\n","\n","num_searches = 5\n","\n","for i in range(num_searches):\n","    # 로그 스케일에서 균등하게 샘플링하여 10의 거듭제곱으로 변환 (효율적)\n","    # np.random.uniform()활용\n","    learning_rate =                      # 여기 10^-3 ~ 10^-1\n","\n","    # 지정된 범위 내에서 정수 무작위 샘플링\n","    # np.random.randint()활용\n","    hidden_size =                        # 여기\n","\n","    print(f\"탐색 {i+1}: 학습률 = {learning_rate:.5f}, 은닉층 크기 = {hidden_size}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"hsTlpEj0IWZW"}},{"cell_type":"code","source":[],"metadata":{"id":"C6innNwC3dEX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**관련 인공지능 개념:**\n","**하이퍼파라미터 튜닝 (Hyperparameter Tuning):** 모델이 스스로 학습하는 가중치(파라미터)와 달리, 개발자가 직접 설정해야 하는 값들을 **하이퍼파라미터**라고 합니다. \\*\\*무작위 탐색(Random Search)\\*\\*은 정해진 범위 내에서 하이퍼파라미터 값들을 무작위로 조합하여 여러 번 모델을 실행해보고, 가장 성능이 좋은 조합을 찾는 방법입니다. 이는 격자 탐색(Grid Search)보다 효율적으로 최적의 값을 찾을 수 있는 경우가 많습니다.\n","\n"],"metadata":{"id":"E7riiQKUIWZW"}},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# 8. [시뮬레이션] 몬테카를로 방법으로 원주율 구하기\n","\n","**문제 제시:**\n","정사각형 안에 원이 내접해 있을 때, 정사각형 안에 무작위로 점을 많이 찍으면 (점의 개수 / 원 안의 점 개수) ≈ (정사각형 넓이 / 원의 넓이)가 됩니다. 이 원리를 이용해 원주율(π)의 근사값을 구해 보세요. (점 10만 개를 찍어 시뮬레이션합니다.)\n","\n","**파이썬 코드:**"],"metadata":{"id":"VZaO6TSshuRR"}},{"cell_type":"code","source":["import numpy as np\n","\n","num_points = 100000\n","\n","# -1과 1 사이의 x, y 좌표를 10만 개씩 무작위로 생성\n","# np.random.uniform()활용\n","x =                         #여기\n","y =                         #여기\n","\n","# 원점으로부터의 거리 계산 (x^2 + y^2)\n","distance_sq =               #여기\n","\n","# 거리가 1 이하인 점들(원 안의 점)의 개수 세기\n","points_in_circle = np.sum(                ) #여기\n","\n","# 원주율 계산: 4 * (원 안의 점 / 전체 점)\n","pi_approx =                                #여기\n","\n","print(f\"총 {num_points}개의 점을 찍어 시뮬레이션\")\n","print(f\"계산된 원주율(π) 근사값: {pi_approx}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"v_l5VQycIWZW"}},{"cell_type":"code","source":[],"metadata":{"id":"KdCNvTRV3eNg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**관련 인공지능 개념:**\n","**몬테카를로 방법 (Monte Carlo Method):** **무작위 샘플링**을 반복하여 복잡한 문제의 해를 근사적으로 구하는 통계적 기법입니다. 직접 계산하기 어려운 적분이나 확률 분포를 시뮬레이션하는 데 널리 사용됩니다. 강화학습에서는 에이전트가 수많은 가상 게임을 플레이(샘플링)하며 최적의 전략을 학습하는 데 이 원리가 적용됩니다.\n","\n"],"metadata":{"id":"2UGqM-jNIWZW"}},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# 9. [확률 분포] 가우시안 노이즈 생성\n","\n","**문제 제시:**\n","현실 세계의 데이터는 다양한 형태의 노이즈를 포함합니다. 가장 일반적인 **가우시안(정규) 분포**를 따르는 노이즈를 생성해 보세요. 평균 0, 표준편차 1인 표준 정규분포에서 1000개의 샘플을 뽑고, 이들의 분포를 간단한 히스토그램으로 확인해 봅시다.\n","\n","**파이썬 코드:**"],"metadata":{"id":"7_CaMrA-h9YB"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# 평균 0, 표준편차 1인 표준 정규분포에서 1000개의 샘플 생성\n","# np.random.randn()활용\n","samples =                        #여기\n","\n","print(f\"생성된 샘플의 평균: {samples.mean():.4f} (이론값: 0)\")\n","print(f\"생성된 샘플의 표준편차: {samples.std():.4f} (이론값: 1)\")\n","\n","# 히스토그램으로 분포 시각화\n","plt.hist(samples, bins=30)\n","plt.title(\"Gaussian Noise Distribution\")\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"id":"_xYnBzIBIWZX"}},{"cell_type":"code","source":[],"metadata":{"id":"QzedueuN3fCO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**관련 인공지능 개념:**\n","**확률 분포 (Probability Distribution):** 인공지능 모델은 종종 데이터가 특정 확률 분포(예: 정규분포)를 따른다고 가정합니다. 예를 들어, \\*\\*생성 모델(GAN)\\*\\*은 무작위 정규분포 노이즈로부터 새로운 이미지나 음악을 창조해냅니다. 이처럼 확률 분포에 대한 이해는 데이터의 특성을 파악하고, 불확실성을 모델링하는 데 필수적입니다.\n","\n"],"metadata":{"id":"DhLAcpTbIWZX"}},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# 10. [데이터 셔플링] 훈련 순서 섞기\n","\n","**문제 제시:**\n","훈련 데이터를 특정 순서(예: 라벨 순서)대로 학습하면 모델이 그 순서에 과적합될 수 있습니다. 학습 전에 데이터의 순서를 \\*\\*무작위로 섞어주는 것(셔플링)\\*\\*이 중요합니다. 0부터 9까지의 숫자로 이루어진 데이터 배열의 순서를 무작위로 섞어 보세요.\n","\n","**파이썬 코드:**"],"metadata":{"id":"TLdLSFUDiAHp"}},{"cell_type":"code","source":["import numpy as np\n","\n","# 0부터 9까지 순서대로 정렬된 데이터\n","data = np.arange(10)\n","print(f\"원본 데이터: {data}\")\n","\n","# 데이터의 순서를 제자리에서(in-place) 무작위로 섞음\n","# np.random.shuffle()활용\n","                           #여기\n","\n","print(f\"셔플된 데이터: {data}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"gszej1EfIWZX"}},{"cell_type":"code","source":[],"metadata":{"id":"dWvPJqCc3ftl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**관련 인공지능 개념:**\n","**셔플링 (Shuffling):** 모델이 데이터의 순서에 담긴 편향을 학습하는 것을 방지하기 위해, 매 학습 주기(epoch)마다 훈련 데이터의 순서를 무작위로 섞어주는 과정입니다. 이는 모델이 데이터의 **본질적인 특징**에 집중하여 학습하도록 돕고, 미니배치를 구성할 때 각 배치가 전체 데이터의 분포를 잘 대표하도록 만드는 효과가 있습니다."],"metadata":{"id":"iLCiXHZpIWZX"}}],"metadata":{"colab":{"provenance":[{"file_id":"18CUE0ndLb1tB_uP0SQn9A-oWuLQ3CakQ","timestamp":1756861065517},{"file_id":"12hvk0I6zizuz8q2Fv_YI954nFxb6_0Vz","timestamp":1756806974229},{"file_id":"1HxKw7cZuMeIv5hXvLsxg5M7i7YXRdLwh","timestamp":1756806729076}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}