{"cells":[{"cell_type":"markdown","source":["활성함수는 특성을 살리는것<br>\n","loss 함수는 실제값과 예측값의 차이를 구하는 함수"],"metadata":{"id":"_2dJxslHrV7C"}},{"cell_type":"code","source":["import numpy as np\n","x = np.array([1, 2, 10, -1])\n","np.set_printoptions(precision=2, suppress=True)\n","ex = np.exp(x)\n","print(ex)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iE25dOk_0lK","executionInfo":{"status":"ok","timestamp":1756289302570,"user_tz":-540,"elapsed":25,"user":{"displayName":"허온","userId":"00077947036147076020"}},"outputId":"ea635ea4-7b78-403c-9bfb-67d4852def7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[    2.72     7.39 22026.47     0.37]\n"]}]},{"cell_type":"markdown","source":["분류 문제는 각 항목일 확률을 구하는 것이다.<br>\n","확률을 구하기 위해서는 음수가 없어야 한다."],"metadata":{"id":"6WebCUJAAkJi"}},{"cell_type":"markdown","source":["자연상수 e를 활성화 함수에 사용하는 이유<br>\n","$e^2$을 미분하면 $e^2$ * $loge$ 가 되는데 이때 $loge$가 사라짐 그래서"],"metadata":{"id":"-AXbeZd7CJ7l"}},{"cell_type":"markdown","source":["활성화 함수의 특징<br>\n","1. 0보다 커야한다.<br>\n","2. 활성화 시켜야 한다.<br>\n","그래서 음수가 없고 입력 값에 따른 y값이 커지는 지수함수를 사용한다.<br>"],"metadata":{"id":"443HbAddrX9y"}},{"cell_type":"code","source":["import math\n","# print(math.exp(100000000000)) # 주석 풀면 에러남 700 이상시\n","print(math.exp(0.00000000001))\n","\n","print(math.log(10000000000))\n","print(math.log(0.00000000001))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gjaKf0rDCjnl","executionInfo":{"status":"ok","timestamp":1756290100370,"user_tz":-540,"elapsed":8,"user":{"displayName":"허온","userId":"00077947036147076020"}},"outputId":"1980a3ae-1c67-4a1b-96eb-aa9bee4ba54a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.00000000001\n","23.025850929940457\n","-25.328436022934504\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","print(np.exp(100000000000))   # 에러가 안나고 inf가 뜸\n","print(math.exp(0.00000000001))\n","\n","print(math.log(10000000000))\n","print(math.log(0.00000000001))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVsJPX2gDEFt","executionInfo":{"status":"ok","timestamp":1756290171680,"user_tz":-540,"elapsed":30,"user":{"displayName":"허온","userId":"00077947036147076020"}},"outputId":"0687d70e-901e-46af-bf85-4c16636f2aec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inf\n","1.00000000001\n","23.025850929940457\n","-25.328436022934504\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-324067095.py:3: RuntimeWarning: overflow encountered in exp\n","  print(np.exp(100000000000))\n"]}]},{"cell_type":"markdown","source":["지수함수는 작은 값이나 큰 값에 불완전함 따라서 log를 사용해서 매우 작은 값을 의미있는 값으로 바꾸기<br>\n","$loge^{-18}=-18$"],"metadata":{"id":"QEo4zxF0DpkX"}},{"cell_type":"markdown","source":["하지만 log를 취하면 원본 값이 달라지기 때문에 다시 지수 함수를 사용<br>"],"metadata":{"id":"Qg08FaJtrXPK"}},{"cell_type":"markdown","source":["로그를 사용하는 이유<br>\n","1. 수치를 안정화 하기 위하여<br>\n","2. 확률을 구할때 곱하거나 나눌때 0이 되거나 작아지는것을 방지하기 위해 왜냐하면 로그의 성질을 활용하여 덧셈과 뺄셈으로 바꿀 수 있다."],"metadata":{"id":"5WfvvE_qGUuB"}},{"cell_type":"markdown","source":["확률의 곱셈을 하는 경우는<br>\n","동전을 돌려서 앞면이 나올경우가 1/2이고 그 다음에 또 앞면이 나올 확률은 앞에 1/2 * 1/2 때문이다."],"metadata":{"id":"tEYYCDEjHjzr"}},{"cell_type":"markdown","source":["-----\n","\n","### 예제 1: 로그와 지수 함수의 관계 이해하기\n","\n","이 코드는 로그와 지수 함수($e^x$)가 서로 역관계에 있다."],"metadata":{"id":"pGpguXo6JSZI"}},{"cell_type":"code","source":["import math\n","\n","print(\"--- 예제 1: 로그와 지수 함수의 역관계 ---\")\n","\n","x = 100\n","\n","log_result = math.log10(x)\n","print(f\"log10({x}) = {log_result}\")\n","\n","# log(e)는 1\n","# 자연로그(밑이 e)를 사용합니다.\n","x = math.e\n","log_result = math.log(x)\n","print(f\"log(e) = {log_result}\")\n","\n","# log 연산의 역산 (exp)\n","\n","y = math.log(100)\n","print(f\"log(100) = {y:.4f}\")\n","print(f\"exp(log(100)) = {math.exp(y):.4f}\")\n","\n","x = 0.01  # 10의 -2제곱\n","log_result = math.log10(x)\n","print(f\"log10(0.01) = {log_result}\")\n","print(f\"10^(log10(0.01)) = {10**log_result}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"0AaEgLHZJSZQ"}},{"cell_type":"code","source":["# numpy로 유사 예제 만들기"],"metadata":{"id":"QzOUKaHFulZX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----\n","\n","### 예제 2: 곱셈을 덧셈으로 바꾸는 로그의 마법\n","\n","로그의 가장 중요한 성질 중 하나는 곱셈을 덧셈으로 바꾸는 것입니다. 이 예제는 복잡한 곱셈을 간단한 덧셈으로 바꿔서 계산하는 과정을 보여주며, 이는 작은 확률 값들의 곱셈을 다루는 방법이 된다."],"metadata":{"id":"VPB0ZkLSJeUH"}},{"cell_type":"code","source":["import math\n","\n","print(\"\\n--- 예제 2: 곱셈을 덧셈으로 ---\")\n","\n","# 원래 곱셈\n","a = 2\n","b = 3\n","c = 4\n","product = a * b * c\n","print(f\"2 * 3 * 4 = {product}\")\n","\n","# 로그를 이용한 계산\n","# log(a * b * c) = log(a) + log(b) + log(c)\n","log_product = math.log(a) + math.log(b) + math.log(c)\n","print(f\"log(2) + log(3) + log(4) = {log_product:.4f}\")\n","\n","# 최종 결과는 exp(로그 합)\n","final_result = math.exp(log_product)\n","print(f\"exp(로그의 합) = {final_result:.4f}\")\n","\n","# 이 방법은 아래와 동일합니다.\n","print(f\"log(2*3*4) = {math.log(2*3*4):.4f}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"1gAzMh67JSZT"}},{"cell_type":"code","source":["# numpy로 유사 예제 만들기"],"metadata":{"id":"GEzzQSRFuvtW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----\n","\n","### 예제 3: 매우 작은 숫자를 로그로 표현하기\n","언더플로우 문제: 일반적인 숫자로 표현하기 어려운 매우 작은 값들을 로그로 다룸"],"metadata":{"id":"GdnyZd8CKDAV"}},{"cell_type":"code","source":["import math\n","\n","print(\"\\n--- 예제 3: 매우 작은 숫자를 로그로 표현 ---\")\n","\n","# 매우 작은 숫자 (0.0000000001)\n","small_num = 1e-10\n","\n","# 로그로 변환하면 다루기 쉬운 음수로 바뀝니다.\n","log_small_num = math.log10(small_num)\n","print(f\"{small_num}의 로그 값: {log_small_num}\")\n","\n","# 더 작은 숫자 (0.0000000000000000001)\n","very_small_num = 1e-19\n","log_very_small_num = math.log10(very_small_num)\n","print(f\"{very_small_num}의 로그 값: {log_very_small_num}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"_GxwQO3ZJSZU"}},{"cell_type":"code","source":["# numpy로 유사 예제 만들기"],"metadata":{"id":"LPStBH_KvDdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----\n","\n","### 예제 4: 확률 곱셈에서 로그의 필요성 시뮬레이션\n","\n","마지막으로, 앞선 예제들의 개념을 결합하여 실제 확률 연산에서 로그가 왜 필요한지 보여주는 간단한 시뮬레이션."],"metadata":{"id":"aRjVW6s1KMfm"}},{"cell_type":"code","source":["import math\n","\n","print(\"\\n--- 예제 4: 언더플로우가 발생하는 확률 곱셈 ---\")\n","\n","# 컴퓨터가 0으로 처리할 만큼 매우 작은 확률 값 리스트\n","# (예: 1e-200은 10의 -200제곱을 의미)\n","underflow_probs = [1e-200, 1e-150, 1e-180]\n","\n","# 1. 일반적인 곱셈 방식 (언더플로우 발생)\n","final_prob_product = 1.0\n","for p in underflow_probs:\n","    final_prob_product *= p\n","\n","# 언더플로우로 인해 결과가 0.0이 됨\n","print(f\"일반 곱셈 결과: {final_prob_product}\")  # 합으로 나눌 경우 0으로 나누게 됨\n","\n","\n","# 2. 로그를 이용한 덧셈 방식 (안정적인 계산)\n","log_probs = [math.log(p) for p in underflow_probs]\n","print(f\"\\n로그 변환된 확률들: {log_probs}\")\n","\n","final_log_sum = sum(log_probs)\n","print(f\"로그 확률들의 합: {final_log_sum:.4f}\") # 0으로 나누진 않음\n","\n","# 최종 확률을 안정적으로 복구 (0이 아닌 매우 작은 값)\n","final_prob_from_log = math.exp(final_log_sum)\n","print(f\"로그를 이용하여 얻은 최종 확률: {final_prob_from_log}\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"id":"EuTswZkdJSZV"}},{"cell_type":"code","source":["# numpy로 유사 예제 만들기"],"metadata":{"id":"l4J4IGUEvGpP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 인공지능에서 로그의 필요성"],"metadata":{"id":"gM5Ch4b9J3i3"}},{"cell_type":"markdown","source":["**인공지능** 분야, 특히 확률 모델링이나 딥러닝에서 발생할 수 있는 **수치적 안정성(numerical stability)** 문제를 해결하는 데 왜 로그(log)가 사용되는지 이해해보자.\n","-----\n","\n","### 예제 1: 너무 작은 확률 값을 곱할 때 발생하는 문제 (언더플로우)\n","\n","이 예제는 일반적인 확률 값을 그대로 곱했을 때 숫자가 너무 작아져서 **컴퓨터가 0으로 인식해 버리는(언더플로우)** 현상을 보여줍니다. 이는 특히 **베이즈 정리나 마르코프 체인 같은 확률 모델에서** 많은 확률을 연속적으로 곱해야 할 때 자주 발생함."],"metadata":{"id":"B3tBMtRMxZxj"}},{"cell_type":"code","source":["import math\n","\n","print(\"--- 일반 확률 값 곱셈 (언더플로우 문제) ---\")\n","\n","# 매우 작은 확률 값들\n","prob_values = [0.00000001, 0.00000002, 0.00000003, 0.00000004, 0.00000005]\n","# 실제로는 훨씬 많은 수의 작은 확률 값들이 곱해지는 상황을 가정합니다.\n","\n","# 일반적인 확률 곱셈\n","combined_prob = 1.0\n","for p in prob_values:\n","    combined_prob *= p\n","    print(f\"현재 곱해진 확률: {combined_prob:.20f}\")\n","\n","print(f\"\\n최종 곱해진 확률 (일반): {combined_prob:.50f}\")\n","print(f\"\\n지수함수 적용할 경우: {math.exp(combined_prob)}\")\n","\n","print(\"\\n--- 로그 확률 덧셈 (수치적 안정성) ---\")\n","\n","# 각 확률 값에 로그를 취합니다. (자연로그 사용)\n","log_prob_values = [math.log(p) for p in prob_values]\n","\n","# 로그 확률들을 더합니다.\n","combined_log_prob = 0.0\n","for log_p in log_prob_values:\n","    combined_log_prob += log_p\n","    print(f\"현재 더해진 로그 확률: {combined_log_prob:.10f}\")\n","\n","# 최종 로그 확률을 다시 원래 확률 값으로 변환\n","final_prob_from_log = math.exp(combined_log_prob)\n","\n","print(f\"\\n최종 더해진 로그 확률: {combined_log_prob:.10f}\")\n","print(f\"로그를 사용하여 얻은 최종 확률: {final_prob_from_log:.50f}\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"id":"QVTIPs-BxZxl"}},{"cell_type":"markdown","source":["**설명:**\n","\n","위 코드를 실행하면 `일반 곱셈 결과`는 `0.00000...000`처럼 0이 되어버리는 것을 볼 수 있다. 컴퓨터가 표현할 수 있는 최소 범위 이하로 숫자가 작아지기 때문이다. 반면, 로그를 취하여 더한 후 다시 `exp()`를 적용한 결과는 0이 아닌 실제 확률에 가까운 매우 작은 값을 정확하게 유지하는 것을 확인할 수 있다. 이는 특히 인공지능 모델의 \\*\\*손실 함수(Loss Function)\\*\\*나 **확률 분포 계산**에서 중요하다. 0이 아닌 실제 확률 값을 정확히 계산해야 모델이 올바르게 학습될 수 있기 때문이다.\n","\n","-----\n","\n","### 예제 2: Log-Sum-Exp 트릭의 구현 (Log-Softmax 또는 정규화에 활용)\n","\n","이 예제는 여러 로그 확률 값을 **안정적으로 더하는** 'Log-Sum-Exp 트릭'을 구현한 것이다. 이는 인공지능, 특히 **분류 모델의 출력(로짓)을 확률로 변환하는 소프트맥스(Softmax) 함수**에서 `log(sum(exp(x)))` 형태의 계산이 필요할 때 **수치적 안정성**을 확보하기 위해 사용된다."],"metadata":{"id":"9F2gkUKAxZxl"}},{"cell_type":"markdown","source":["# Softmax함수\n","\n","소프트맥스(Softmax) 함수의 식은 다음과 같습니다.\n","\n","벡터 $\\mathbf{z} = (z_1, z_2, ..., z_K)$에 대한 소프트맥스 함수 $S$의 $i$번째 결과값 $S_i$는 다음과 같이 계산됩니다.\n","\n","$$S_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n","\n","---"],"metadata":{"id":"rEUMoe1o4FzX"}},{"cell_type":"markdown","source":["### ## LogSoftmax의 공식\n","\n","벡터 $\\mathbf{z} = (z_1, z_2, ..., z_K)$에 대한 로그소프트맥스 함수 $LS$의 $i$번째 결과값 $LS_i$는 다음과 같이 계산됩니다.\n","\n","$$LS_i = \\log\\left(\\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\\right) = z_i - \\log\\left(\\sum_{j=1}^{K} e^{z_j}\\right)$$\n"],"metadata":{"id":"BWtVMvOw4VCy"}},{"cell_type":"code","source":["import math\n","import numpy as np\n","\n","print(\"--- Log-Sum-Exp 트릭 구현 ---\")\n","\n","# 인공지능 모델의 '로짓(logits)' 또는 '로그 확률'이라고 가정할 수 있는 값들\n","# 보통 모델의 마지막 레이어에서 나오는 정규화되지 않은 점수입니다.\n","log_probs = np.array([-10.0, -12.0, -9.0, -15.0]) # 예시 로그 확률 값들\n","\n","print(f\"원래 로그 확률 값들: {log_probs}\")\n","\n","# 1. 수동으로 Log-Sum-Exp 계산 (설명된 과정)\n","print(\"\\n--- 수동 Log-Sum-Exp 계산 ---\")\n","max_log_prob = np.max(log_probs)\n","print(f\"최대 로그 확률 (max_log_prob): {max_log_prob}\")\n","\n","# 각 로그 확률에서 max_log_prob을 뺌(모두 0 이하의 값으로 바꿈)\n","shifted_log_probs = log_probs - max_log_prob\n","print(f\"쉬프트된 로그 확률 (log_probs - max_log_prob): {shifted_log_probs}\")\n","\n","# exp를 취하여 원래 스케일로 복원 (언더플로우 방지)(모두 1이하)\n","exp_shifted_probs = np.exp(shifted_log_probs)\n","print(f\"exp(쉬프트된 로그 확률): {exp_shifted_probs}\")\n","\n","# exp_shifted_probs 값들을 모두 더함\n","sum_exp_shifted_probs = np.sum(exp_shifted_probs)\n","print(f\"sum(exp(쉬프트된 로그 확률)): {sum_exp_shifted_probs}\")\n","\n","# 최종적으로 max_log_prob을 다시 더하고 로그를 취함\n","log_sum_exp_manual = max_log_prob + math.log(sum_exp_shifted_probs)\n","print(f\"수동 계산 Log-Sum-Exp 결과: {log_sum_exp_manual:.10f}\")\n","\n","# 2. NumPy의 logaddexp 함수 사용 (실제 라이브러리 활용)\n","print(\"\\n--- NumPy의 logaddexp 함수 사용 ---\")\n","\n","# NumPy는 Log-Sum-Exp를 위한 최적화된 함수를 제공합니다.\n","# logaddexp는 두 개의 로그 값을 더할 때 사용하며,\n","# 여러 개를 더할 때는 reduce 함수와 함께 사용합니다.\n","log_sum_exp_numpy = np.logaddexp.reduce(log_probs)\n","print(f\"NumPy logaddexp 결과: {log_sum_exp_numpy:.10f}\")\n","\n","# 결과 비교\n","print(\"\\n--- 결과 비교 ---\")\n","print(f\"수동 계산 Log-Sum-Exp: {log_sum_exp_manual:.10f}\")\n","print(f\"NumPy logaddexp 결과: {log_sum_exp_numpy:.10f}\")\n","\n","# 최종 결과가 확률 합의 로그 값임을 확인\n","# 예를 들어, 이 값이 log(Z)라고 하면, Z는 정규화 상수 역할을 합니다.\n","# log-softmax는 log(exp(x_i) / sum(exp(x_j))) = x_i - log(sum(exp(x_j)))\n","# 여기서 log(sum(exp(x_j)))이 바로 이 log_sum_exp_numpy 값입니다."],"outputs":[],"execution_count":null,"metadata":{"id":"eQf0kvRexZxm"}},{"cell_type":"markdown","source":["**설명:**\n","\n","`Log-Sum-Exp` 트릭은 단순히 `exp(log_prob1) + exp(log_prob2)`를 계산하는 대신, 가장 큰 로그 확률 값을 빼서 모든 값들이 0보다 작거나 같은 작은 음수가 되도록 조정한 후 `exp`를 취한다. 이렇게 하면 `exp()`를 적용할 때 \\*\\*오버플로우(overflow, 숫자가 너무 커져서 컴퓨터가 표현할 수 없게 되는 현상)\\*\\*나 언더플로우를 방지할 수 있다. 예를 들어, `exp(1000)`은 컴퓨터가 표현할 수 있는 범위를 훨씬 넘어설 수 있지만, `exp(-1)`이나 `exp(0)`은 안정적으로 계산된다.\n","\n","NumPy 같은 라이브러리에는 이 트릭이 `logaddexp`나 `logsumexp`와 같은 함수로 이미 구현되어 있다. 이는 딥러닝 프레임워크(TensorFlow, PyTorch 등)에서 **로그 소프트맥스(Log-Softmax)** 함수를 구현할 때 내부적으로 사용되어, 모델이 안정적으로 학습되고 예측을 수행할 수 있도록 돕는다.\n","\n","-----\n","\n"],"metadata":{"id":"iklkhIwjxZxm"}},{"cell_type":"markdown","source":[],"metadata":{"id":"ryCDZUziIp3h"}},{"cell_type":"markdown","source":["# 인공지능 알고리즘에서 로그 함수의 적용 및 필요성.ppt에서의 코드 이해"],"metadata":{"id":"J6tvXM5zpeKH"}},{"cell_type":"markdown","source":["**(#5) 확률 곱셈과 언더플로우 문제**"],"metadata":{"id":"jMcZiQuisJ_y"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"ZkcDffmzpnky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","x = np.array([0.001, 0.0005, 0.0002])\n","# 3개의 확률값의 크기(p)\n","p = x[0] * x[1] * x[2]\n","print(p)\n"],"metadata":{"id":"N8dw3DmtppVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["log_p = np.log(x[0]) + np.log(x[1]) + np.log(x[2])\n","print(log_p)"],"metadata":{"id":"l1iB8MKFqLuI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.exp(log_p)) #1e-10"],"metadata":{"id":"0jOfW9znrkZI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**(#7) softmax 함수와 오버플로우 문제**"],"metadata":{"id":"d7T3uirDr9f8"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"Haa02gCzrusf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = np.array([2.0, 1.0, 0.1])\n","exp_x = np.exp(x)\n","print(exp_x)\n","print(np.sum(exp_x))\n","#softmax결과는?\n","\n","softm = exp_x / np.sum(exp_x)\n","print(np.round(softm,3))"],"metadata":{"id":"sSUQqTfusaES"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 만약, 입력 값이 [20.0, 10.0, 5.0]으로 커지면?"],"metadata":{"id":"7j-bngLyst9b"}},{"cell_type":"code","source":["x2 = np.array([20.0, 1000.0, 5.0])\n","exp_x2 = np.exp(x2)\n","print(exp_x2)\n","print(np.sum(exp_x2))\n","\n","#softmax결과는?\n","softm2 = exp_x2 / np.sum(exp_x2)\n","print(np.round(softm2,3))"],"metadata":{"id":"PZo9ZGM-stWo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**(#8) Log-Sum-Exp의 원리**"],"metadata":{"id":"LqGVTu6KueMi"}},{"cell_type":"code","source":["# LSE의 원리만 파악하기 위해\n","x3 = np.array([-10, -12, -9, -15])\n","c = np.max(x3)\n","print(c)\n","\n","#수학적 항등식의 비교\n","lse1 = np.log(np.sum(np.exp(x3)))\n","lse2 = c+np.log(np.sum(np.exp(x3-c)))\n","print(lse1==lse2)"],"metadata":{"id":"mIxvohA0xLP9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**(#9) 실습: Log-Sum-Exp트릭 구현하기**"],"metadata":{"id":"78b4RH7tyv9X"}},{"cell_type":"code","source":["print(lse2)"],"metadata":{"id":"Wya6LoIpzEgv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.logaddexp.reduce(x3))"],"metadata":{"id":"r_LHz6dGzVRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pIe6c1wozbUB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"qSSpZn0u2VWy"}},{"cell_type":"markdown","source":["# PyTorch로 **LogSoftmax**와 **NLLLoss**를 직접 구현하고, 이를 PyTorch의 내장 함수인 `nn.CrossEntropyLoss`와 비교하여 수치적 안정성과 효율성을 확인하는 코드  \n"],"metadata":{"id":"Kl6wF3K72gd1"}},{"cell_type":"markdown","source":["### \\#\\#  핵심 개념: 왜 LogSoftmax를 사용할까요?\n","\n","일반적인 Softmax 함수는 입력 값을 0과 1 사이의 확률 값으로 변한다. 하지만 입력값(logit)이 매우 크거나 작을 경우, 심각한 수치적 오류를 발생시킬 수 있다.\n","\n","  * **오버플로우 (Overflow)**: `exp(x)`에서 `x`가 매우 큰 양수이면 (예: 1000), 그 결과는 컴퓨터가 표현할 수 있는 한계를 넘어 무한대(`inf`)가 된다.\n","  * **언더플로우 (Underflow)**: `x`가 매우 작은 **음수이면** (예: -1000), `exp(x)`는 0에 매우 가까워져 결국 0으로 처리됩니다. 이후 `log(0)`을 계산하면 음의 무한대(`-inf`)가 되어 계산이 불가능해진다.\n","\n","**LogSoftmax**는 \\*\\*\"Log-Sum-Exp 트릭\"\\*\\*이라는 기법을 내부적으로 사용하여 이러한 문제를 해결한다. 입력값에서 최댓값을 빼준 뒤 `exp`를 계산함으로써 `exp`의 결과가 항상 1 이하가 되도록 만들어 오버플로우를 방지하고 수치적 안정성을 크게 향상시킨다.\n","\n","PyTorch의 `nn.CrossEntropyLoss`는 이 `nn.LogSoftmax()`와 `nn.NLLLoss()`를 합쳐놓은, 매우 효율적이고 안정적인 함수이다. 따라서 실제 모델을 구현할 때는 항상 `nn.CrossEntropyLoss`를 사용하는 것이 좋다.\n","\n","\n"],"metadata":{"id":"FcntFymC2vk5"}},{"cell_type":"code","source":["import numpy as np\n","print(np.exp(-1000))"],"metadata":{"id":"NwhaO0cW80wu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# ✅1. LogSoftmax와 NLLLoss 라이브러리로 구현\n"],"metadata":{"id":"pC_AIUlE8u5g"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","# 샘플 데이터 생성\n","# 3개의 샘플(batch), 5개의 클래스(class)를 가정\n","logits = torch.randn(3, 5)\n","targets = torch.tensor([1, 0, 4]) # 각 샘플의 정답 클래스 인덱스\n","\n","print(\"--- 입력 데이터 ---\")\n","print(f\"Logits (모델의 원시 출력):\\n{logits}\\n\")\n","print(f\"Targets (정답 레이블):\\n{targets}\\n\")\n","\n","log_softmax = nn.LogSoftmax(dim=1)\n","log_probs = log_softmax(logits)\n","print(\"--- LogSoftmax 결과 ---\")\n","print(f\"LogSoftmax 결과:\\n{log_probs}\\n\")\n","\n","# 2. NLLLoss (Negative Log Likelihood Loss) 적용\n","# NLLLoss 객체 생성\n","nll_loss_criterion = nn.NLLLoss()\n","\n","# LogSoftmax 결과와 정답 타겟을 이용해 Loss 계산\n","loss = nll_loss_criterion(log_probs, targets)\n","\n","print(\"--- NLLLoss 결과 ---\")\n","print(f\"최종 NLLLoss 값: {loss.item():.6f}\")"],"metadata":{"id":"lkMn8bXr2iIA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PyTorch 내장 CrossEntropyLoss 사용\n","# nn.CrossEntropyLoss는 LogSoftmax와 NLLLoss를 한번에 처리함\n","# 따라서 입력으로 raw logits를 넣어주어야 함\n","criterion = nn.CrossEntropyLoss()\n","loss_pytorch = criterion(logits, targets)\n","\n","print(\"\\n--- PyTorch 내장 함수 결과 ---\")\n","print(f\"PyTorch Cross Entropy Loss 결과: {loss_pytorch.item():.6f}\\n\")\n"],"metadata":{"id":"_PUCOa316jw1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----\n","### \\#\\# ✅ 2. LogSoftmax와 NLLLoss 수동 구현\n","\n","먼저 PyTorch를 이용해 LogSoftmax와 NLLLoss의 핵심 로직을 직접 코드로 구현"],"metadata":{"id":"U3tCDHhl6vjU"}},{"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","# 샘플 데이터 생성\n","# 3개의 샘플(batch), 5개의 클래스(class)를 가정\n","logits = torch.randn(3, 5)\n","targets = torch.tensor([1, 0, 4]) # 각 샘플의 정답 클래스 인덱스\n","\n","print(\"--- 입력 데이터 ---\")\n","print(f\"Logits (모델의 원시 출력):\\n{logits}\\n\")\n","print(f\"Targets (정답 레이블):\\n{targets}\\n\")\n","\n","\n","# 수동 LogSoftmax 구현 (Log-Sum-Exp 트릭 적용)\n","def my_log_softmax(x):\n","    # x의 각 행(row)에서 최댓값을 찾음 (수치 안정성을 위해)\n","    # keepdim=True를 통해 차원을 유지하여 브로드캐스팅이 가능하게 함\n","    max_x = torch.max(x, dim=1, keepdim=True).values\n","\n","    # 여기 완성\n","    exp_x_shifted = torch.exp(             )\n","\n","    # 여기 완성\n","    sum_exp = torch.sum(      , dim=1, keepdim=True)\n","    log_sum_exp = torch.log(         )\n","\n","    # LogSoftmax 최종 결과 반환\n","    return (                   ) #LogSoftMax\n","\n","# 수동 NLLLoss 구현\n","def my_nll_loss(log_probs, targets):\n","    num_samples = targets.shape[0]\n","\n","    selected_log_probs = log_probs[range(num_samples), targets]\n","\n","    # 선택된 log 확률들의 평균에 음수를 취함\n","    return -torch.mean(selected_log_probs)\n","\n","# 수동으로 구현한 함수들을 통해 Cross Entropy Loss 계산\n","log_probs_manual = my_log_softmax(logits)\n","loss_manual = my_nll_loss(log_probs_manual, targets)\n","\n","print(\"--- 수동 구현 결과 ---\")\n","print(f\"My LogSoftmax 결과:\\n{log_probs_manual}\\n\")\n","print(f\"My Cross Entropy Loss 결과: {loss_manual.item():.6f}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"Lb4Tgp4Z2b8H"}},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# ✅ 3. PyTorch 내장 CrossEntropyLoss와 비교\n","\n","이제 위에서 수동으로 계산한 값과 PyTorch에 내장된 `nn.CrossEntropyLoss`의 결과를 비교"],"metadata":{"id":"ytyfU4XO4Dgm"}},{"cell_type":"code","source":[],"metadata":{"id":"3J6lVefI26DM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nn.CrossEntropyLoss는 LogSoftmax와 NLLLoss를 한번에 처리함\n","# 여기 완성\n","criterion = (                   )\n","loss_pytorch = criterion(           )\n","\n","print(\"\\n--- PyTorch 내장 함수 결과 ---\")\n","print(f\"PyTorch Cross Entropy Loss 결과: {loss_pytorch.item():.6f}\\n\")\n","\n","# 두 결과 비교\n","print(\"--- 최종 비교 ---\")\n","print(f\"수동 구현 Loss: {loss_manual.item():.6f}\")\n","print(f\"PyTorch 내장 Loss: {loss_pytorch.item():.6f}\")\n","print(f\"두 Loss 값의 차이: {torch.abs(loss_manual - loss_pytorch).item():.8f}\")\n","print(\"\\n결과가 거의 동일한 것을 확인할 수 있습니다! (미세한 부동소수점 차이만 존재)\")"],"outputs":[],"execution_count":null,"metadata":{"id":"pEzETP-l2b8K"}},{"cell_type":"markdown","source":["결과를 보면, 우리가 직접 구현한 Loss 값과 PyTorch의 내장 함수가 계산한 Loss 값이 소수점 아래에서 발생하는 미세한 차이를 제외하고는 거의 동일함을 알 수 있습니다."],"metadata":{"id":"heqRUOB44RdK"}},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# ✅ 4. 수치적 안정성 테스트: 왜 Log-Sum-Exp가 중요한가?\n","\n","이번에는 Log-Sum-Exp 트릭의 중요성을 확인하기 위해 **매우 큰 값**을 가진 `logits`를 사용해 보겠습니다."],"metadata":{"id":"br0GjAYH4buy"}},{"cell_type":"code","source":[],"metadata":{"id":"PPv3s5C34JQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 불안정한 (naive) Softmax + log 구현\n","def naive_softmax_then_log(x):\n","    softmax_output = torch.exp(x) / torch.sum(torch.exp(x), dim=1, keepdim=True)\n","    return torch.log(softmax_output)\n","\n","# 매우 큰 값을 가진 logits 생성\n","large_logits = torch.tensor([[1000., 1002., 999.]])\n","large_targets = torch.tensor([1])\n","\n","print(\"\\n\\n--- 수치적 안정성 테스트 (매우 큰 Logits) ---\")\n","\n","# 1. 불안정한 Naive 방식\n","try:\n","    log_probs_naive = naive_softmax_then_log(large_logits)\n","    print(f\"\\nNaive 방식의 LogSoftmax 결과:\\n{log_probs_naive}\")\n","    # exp(1000)이 inf가 되면서 계산 결과가 NaN(Not a Number)이 됨\n","except Exception as e:\n","    print(f\"\\nNaive 방식 실행 중 오류 발생: {e}\")\n","\n","# 2. 안정적인 수동 구현 방식 (Log-Sum-Exp 트릭 사용)\n","log_probs_stable = my_log_softmax(large_logits)\n","loss_stable = my_nll_loss(log_probs_stable, large_targets)\n","print(f\"\\n안정적인 방식의 LogSoftmax 결과:\\n{log_probs_stable}\")\n","print(f\"안정적인 방식의 Loss 결과: {loss_stable.item():.6f}\")\n","\n","# 3. PyTorch 내장 함수\n","loss_pytorch_stable = criterion(large_logits, large_targets)\n","print(f\"\\nPyTorch 내장 함수의 Loss 결과: {loss_pytorch_stable.item():.6f}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"9r6rY0n62b8L"}},{"cell_type":"code","source":[],"metadata":{"id":"GuuPos5L4gEw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 퀴즈\n","\n","**문제**: 한글 영화 리뷰가 '긍정'일 확률과 '부정'일 확률을 **Log-Sum-Exp**를 활용해 안정적으로 계산하세요.\n","\n","**주어진 문장**: \"이 영화 최고의 인생 영화\"\n","\n","**단어 점수표**:\n","\n","| 단어 | 긍정 점수 (Logit) | 부정 점수 (Logit) |\n","| :--- | :--- | :--- |\n","| 이 | 0.1 | -0.1 |\n","| 영화 | 1.5 | 1.0 |\n","| 최고의 | 5.0 | -8.0 |\n","| 인생 | 4.5 | 3.1 |\n","\n","**요구사항**:\n","\n","1.  문장 전체의 '최종 긍정 점수'와 '최종 부정 점수'를 각 단어 점수의 합으로 구하세요.\n","2.  최종 점수가 매우 클 경우를 대비하여, **Log-Sum-Exp**를 활용해 이 문장이 '긍정'일 확률과 '부정'일 확률을 안정적으로 계산하는 코드를 작성하세요.\n","3.  즉, LogSoftmax를 Log-Sum-Exp를 활용해 구한 뒤, exp로 (팁)\n"],"metadata":{"id":"0Jxgc_EGErfK"}},{"cell_type":"code","source":["import math\n","import numpy as np\n","\n","positive = [0.1, 1.5, 5.0, 4.5]\n","negative = [-0.1, 1.0, -8.0, 3.1]\n","\n","positive_sum = sum(positive)\n","negative_sum = sum(negative)\n","\n","x = np.array([positive_sum, negative_sum])\n","\n","def log_sum_exp(x):\n","  max_value = max(x)\n","  return max_value + math.log(sum(math.exp(x - max_value)))\n","\n","lse_value = log_sum_exp(x)\n","\n","positive_value = positive_sum - lse_value\n","negative_value = negative_sum = lse_value\n","print(math.exp(positive_value), math.exp(negative_value))"],"metadata":{"id":"RyZ0Zc73Er7v","colab":{"base_uri":"https://localhost:8080/","height":342},"executionInfo":{"status":"error","timestamp":1756896404058,"user_tz":-540,"elapsed":20,"user":{"displayName":"허온","userId":"00077947036147076020"}},"outputId":"a98c15f1-bc3a-4fa6-848e-0d57db6e4c19"},"execution_count":7,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"only length-1 arrays can be converted to Python scalars","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-961166935.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mlse_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_sum_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpositive_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive_sum\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlse_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-961166935.py\u001b[0m in \u001b[0;36mlog_sum_exp\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog_sum_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlse_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_sum_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1YlJ7vaixPyN-H5mvNnvPC4VBE58DduvL","timestamp":1755587360299}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}