{"cells":[{"cell_type":"markdown","source":["\n","\n","### **예제 1: 함수의 미분(Derivative)과 기울기**\n","\n","이 예제는 이차 함수 $f(x) = x^2$의 미분 값인 $f'(x) = 2x$를 이용해 특정 지점(x=3)에서의 기울기를 계산하는 방법을 보여줍니다. 경사하강법은 이 기울기를 이용해 함수의 최솟값을 찾습니다."],"metadata":{"id":"EGnpRfX8L5L7"}},{"cell_type":"code","source":["# 함수 정의\n","def my_function(x):\n","    return x**2\n","\n","# 미분 함수 정의\n","def derivative_of_my_function(x):\n","    return      #여기\n","\n","# x = 3에서의 기울기 계산\n","x_value = 3\n","slope = derivative_of_my_function(x_value)\n","\n","print(f\"함수 f(x) = x^2에서 x = {x_value}일 때의 기울기(미분값)는 {slope}입니다.\")"],"outputs":[],"execution_count":null,"metadata":{"id":"GDPxI52XL5MA"}},{"cell_type":"markdown","source":["-----\n","\n","### **예제 2: 기울기를 이용한 값 업데이트**\n","\n","경사하강법은 기울기를 이용해 현재 위치에서 최솟값 방향으로 조금씩 이동합니다. 이 예제는 현재 위치 $x$와 기울기 $f'(x)$를 이용해 다음 위치를 업데이트하는 간단한 과정을 보여줍니다. 여기서 학습률(Learning Rate)은 한 번에 이동하는 크기를 결정합니다.  \n","x초기값을 5, 학습률을 0.1로 해서 x의 다음 위치를 추정해보자"],"metadata":{"id":"UZlt3csqL5MB"}},{"cell_type":"code","source":["# 미분 함수 정의 (예제 1과 동일)\n","def derivative_of_my_function(x):\n","    return            #여기\n","\n","# 초기 위치와 학습률 설정\n","x_current = #여기\n","learning_rate = #여기\n","\n","# 기울기 계산\n","slope_at_current_x = derivative_of_my_function(x_current)\n","\n","# 다음 위치 업데이트\n","x_next = #여기\n","\n","print(f\"현재 위치는 {x_current}입니다.\")\n","print(f\"이 위치에서의 기울기는 {slope_at_current_x}입니다.\")\n","print(f\"학습률 0.1로 업데이트된 다음 위치는 {x_next}입니다.\")"],"outputs":[],"execution_count":null,"metadata":{"id":"OF7eQotRL5MC"}},{"cell_type":"markdown","source":["-----\n","\n","### **예제 3: 경사하강법 반복 프로세스 시뮬레이션**\n","\n","이 예제는 예제 2의 업데이트 과정을 여러 번 반복하여 함수 $f(x) = x^2$의 최솟값(x=0)에 가까워지는 과정을 시뮬레이션합니다. 경사하강법은 이 반복적인 업데이트를 통해 최적의 해를 찾아갑니다. 이때 x초기값을 10, 학습률을 0.1, 반복횟수를 5,10,30,50으로 해본다"],"metadata":{"id":"DegWYfPqL5MC"}},{"cell_type":"code","source":["# 미분 함수 정의 (예제 1과 동일)\n","def derivative_of_my_function(x):\n","    return #여기\n","\n","# 초기 위치와 학습률, 반복 횟수 설정\n","x = #여기\n","learning_rate = #여기\n","iterations = #여기  # 10, 30, 40, 50 해본다\n","\n","print(\"경사하강법 반복 과정:\")\n","print(f\"초기 위치: x = {x}\")\n","print(\"-\" * 30)\n","\n","for i in range(iterations):\n","    # 기울기 계산\n","    slope = #여기\n","\n","    # 위치 업데이트\n","    x = #여기\n","\n","    # 현재 상태 출력\n","    print(f\"반복 {i+1}: 기울기 = {slope:.2f}, 업데이트된 x = {x:.2f}\")\n","\n","print(\"-\" * 30)\n","print(f\"최종 위치: x = {x:.2f}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"zNf1WtvqL5MD"}},{"cell_type":"markdown","source":["-----\n","\n","### **예제 4: 더 복잡한 함수의 최소값 찾기**\n","\n","이 예제는 경사하강법이 $f(x) = x^4 - 3x^2 + 2$와 같은 더 복잡한 함수의 최소값을 찾는 데 어떻게 사용될 수 있는지 보여줍니다. 미분함수를 정의하고 x초기값을 2.5, 학습률을 0.05, 반복을 20,50,100으로 해보자"],"metadata":{"id":"rfamutmbL5MD"}},{"cell_type":"code","source":["# 함수 정의\n","def my_complex_function(x):\n","    return x**4 - 3 * x**2 + 2\n","\n","# 미분 함수 정의\n","def derivative_of_complex_function(x):\n","    return   #여기\n","\n","# 초기 위치와 하이퍼파라미터 설정\n","x = #여기\n","learning_rate = #여기\n","iterations = 20 # 50, 100 해보기\n","\n","print(\"복잡한 함수에 대한 경사하강법 반복 과정:\")\n","print(f\"초기 위치: x = {x:.2f}\")\n","print(\"-\" * 30)\n","\n","for i in range(iterations):\n","    # 기울기 계산\n","    slope = #여기\n","    # 위치 업데이트\n","    x = #여기\n","\n","    # 10회 마다 출력\n","    if        #여기\n","      print(f\"반복 {i+1}: 기울기 = {slope:.2f}, 업데이트된 x = {x:.2f}\")\n","\n","print(\"-\" * 30)\n","print(f\"최종 위치: x = {x:.2f}\")\n","print(f\"최종 함수 값: y = {my_complex_function(x):.2f}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"E5pIo4KTL5ME"}},{"cell_type":"markdown","source":["이 네 가지 예제를 통해 경사하강법의 기본 원리인 **기울기 계산**, **학습률을 이용한 업데이트**, 그리고 **반복을 통한 최적화** 개념을 파악할 수 있을 거예요. 이 과정을 이해했다면, 이제 실제 데이터를 이용한 **선형 회귀**와 같은 문제에 경사하강법을 적용하는 방법을 배워볼 수 있습니다."],"metadata":{"id":"NTSsc9V8L5MF"}},{"cell_type":"markdown","source":["앞서 배운 기본 개념을 바탕으로, \\*\\*경사하강법(Gradient Descent)\\*\\*을 실제로 구현한 파이썬 코드 예제 2가지는 다음과 같다.\n","\n","첫 번째 예제는 우리가 앞서 다루었던 간단한 함수 $f(x) = x^2$의 최솟값을 찾는 과정이며, 두 번째 예제는 **선형 회귀(Linear Regression)** 문제를 경사하강법으로 해결하는 실용적인 예제입니다.\n","\n","-----\n","\n","### **예제 5: 1차원 함수의 최솟값 찾기**\n","\n","이 코드는 \\*\\*손실 함수(Loss Function)\\*\\*가 $f(x) = x^2$이고, 목표는 이 함수의 최솟값인 $x=0$을 찾는 것입니다. 경사하강법은 이 손실 함수의 기울기를 따라 \\*\\*손실(Loss)\\*\\*이 최소화되는 지점으로 점차 이동합니다."],"metadata":{"id":"Kqw7hz2QNDlZ"}},{"cell_type":"code","source":["import numpy as np\n","\n","def gradient_descent_1d(learning_rate, n_iterations, initial_x):\n","\n","    x = initial_x\n","    history = [x]\n","\n","    # f(x) = x^2의 미분 함수 (기울기)\n","    def derivative(x):\n","        return #여기\n","\n","    for i in range(n_iterations):\n","        # 기울기 계산\n","        gradient = #여기\n","\n","        # 새로운 x 값으로 업데이트\n","        x = #여기\n","        history.append(x)\n","\n","    return history\n","\n","# 경사하강법 실행\n","learning_rate = 0.1\n","n_iterations = 10\n","initial_x = 5.0\n","\n","x_history = #여기\n","\n","print(\"1차원 경사하강법 실행 결과\")\n","print(\"-\" * 30)\n","for i, x_val in enumerate(x_history):\n","    if i % 2 == 0:  # 짝수 횟수 출력\n","        print(f\"반복 {i+1}: x = {x_val:.4f}, 손실 = {x_val**2:.4f}\")  # y=x^2 이 손실함수 이므로\n","\n","print(\"-\" * 30)\n","print(f\"최종 x 값: {x_history[-1]:.4f}\")\n","print(f\"최종 손실: {x_history[-1]**2:.4f}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"H_NdIyNeNDlg"}},{"cell_type":"markdown","source":["-----\n","\n","### **예제 6: 선형 회귀(Linear Regression) 문제 해결**\n","\n","이 코드는 실제 데이터를 사용해 선형 회귀 모델의 최적의 기울기($w$)와 절편($b$)을 찾는 과정을 보여줍니다. **손실 함수**는 \\*\\*평균 제곱 오차(Mean Squared Error)\\*\\*를 사용하며, 이 손실을 최소화하는 $w$와 $b$를 경사하강법으로 찾아냅니다."],"metadata":{"id":"mCEZWP9WNDlj"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def gradient_descent_linear_regression(X, y, learning_rate, n_iterations):\n","    \"\"\"\n","    경사하강법을 사용한 선형 회귀 모델 훈련\n","\n","    Args:\n","        X (np.array): 입력 데이터\n","        y (np.array): 목표 데이터\n","        learning_rate (float): 학습률\n","        n_iterations (int): 반복 횟수\n","\n","    Returns:\n","        tuple: 최종 기울기(w)와 절편(b)\n","    \"\"\"\n","    m = len(y)\n","\n","    # 가중치(기울기) w와 편향(절편) b를 0으로 초기화\n","    w = 0\n","    b = 0\n","\n","    # 손실 기록\n","    loss_history = []\n","\n","    for i in range(n_iterations):\n","        # 예측값 계산 (y_hat = wx + b)\n","        y_hat = w * X + b\n","\n","        # 손실 함수 (MSE)의 미분 값(기울기) 계산\n","        # 손실함수 L을 w로 편미분\n","        dw = #여기\n","        # 손실함수 L을 b로 편미분\n","        db = #여기\n","\n","        # w와 b 업데이트\n","        w = #여기\n","        b = #여기\n","\n","        # 손실 계산\n","        loss = (1/(2*m)) * np.sum((y_hat - y)**2)\n","        loss_history.append(loss)\n","\n","    # 손실 그래프 출력\n","    plt.plot(np.arange(n_iterations), loss_history)\n","    plt.title(\"Loss History\")\n","    plt.xlabel(\"Iteration\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()\n","\n","    return w, b\n","\n","# 훈련 데이터 생성\n","np.random.seed(42)\n","X = np.random.rand(100, 1) * 10\n","y = 3 * X + 4 + np.random.randn(100, 1) * 2\n","\n","# 경사하강법 실행\n","learning_rate = 0.01\n","n_iterations = 1000\n","\n","final_w, final_b = gradient_descent_linear_regression(X, y, learning_rate, n_iterations)\n","\n","print(\"선형 회귀 경사하강법 실행 결과\")\n","print(\"-\" * 30)\n","print(f\"최종 기울기(w): {final_w:.4f}\")\n","print(f\"최종 절편(b): {final_b:.4f}\")\n","\n","# 예측 결과 시각화\n","plt.scatter(X, y, label=\"Original Data\")\n","plt.plot(X, final_w * X + final_b, color='red', label=\"Fitted Line\")\n","plt.title(\"Linear Regression with Gradient Descent\")\n","plt.xlabel(\"X\")\n","plt.ylabel(\"y\")\n","plt.legend()\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"id":"eQgy7vIRNDlj"}},{"cell_type":"markdown","source":["## 예제 7: 1차원 함수의 최솟값 찾기 ($f(x) = x^2 - 4x + 5$)\n","\n","$f(x) = x^2$보다 조금 더 복잡한 이차 함수의 최솟값을 경사 하강법으로 찾아가는 예제입니다. 이 함수의 최솟값은 $x=2$일 때 1입니다.\n"],"metadata":{"id":"VI6RiSJF14i2"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# 손실 함수: f(x) = x^2 - 4x + 5\n","def loss_function(x):\n","    return x**2 - 4*x + 5\n","\n","# 손실 함수의 미분(기울기): f'(x) = 2x - 4\n","def derivative(x):\n","    return #여기\n","\n","# 경사 하강법 실행\n","learning_rate = 0.1\n","n_iterations = 30\n","initial_x = 10.0\n","\n","x = initial_x\n","x_history = [x]\n","loss_history = [loss_function(x)]\n","\n","for i in range(n_iterations):\n","    gradient = #여기\n","    x = #여기\n","    x_history.append(x)\n","    loss_history.append(#여기)\n","\n","print(f\"최종 x 값: {x:.4f}\")\n","print(f\"최종 손실 값: {loss_function(x):.4f}\")\n","\n","# 시각화\n","plt.figure(figsize=(12, 5))\n","\n","# 손실 함수 그래프\n","plt.subplot(1, 2, 1)\n","x_vals = np.linspace(-2, 12, 100)\n","y_vals = loss_function(x_vals)\n","plt.plot(x_vals, y_vals, label='f(x) = x^2 - 4x + 5')\n","plt.plot(x_history, loss_history, 'ro-', markersize=4, label='Gradient Descent Path')\n","plt.title('Loss Function & Gradient Descent Path')\n","plt.xlabel('x')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid(True)\n","\n","# 반복에 따른 x값 변화\n","plt.subplot(1, 2, 2)\n","plt.plot(range(n_iterations + 1), x_history, 'bo-')\n","plt.title('Change of x value over iterations')\n","plt.xlabel('Iteration')\n","plt.ylabel('x value')\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TN6pn16cNbOQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 예제 8: 다변수 함수(2차원)의 최솟값 찾기\n","\n","이번에는 두 개의 변수($x_1, x_2$)를 갖는 함수의 최솟값을 찾는 예제입니다. 각 변수에 대해 편미분을 사용하여 기울기를 계산하고 업데이트합니다. 손실함수는 $x_1^2 + 2x_2^2$ 이다."],"metadata":{"id":"HGCaM5bT2Lmd"}},{"cell_type":"code","source":["import numpy as np\n","\n","# 손실 함수: f(x1, x2) = x1^2 + 2*x2^2\n","\n","def derivatives(x1, x2):\n","    dw1 = #여기\n","    dw2 = #여기\n","    return dw1, dw2\n","\n","# 경사 하강법 실행\n","learning_rate = 0.1\n","n_iterations = 20\n","params = np.array([5.0, 5.0]) # [x1, x2] 시작점\n","history = [params]\n","\n","for i in range(n_iterations):\n","    dw1, dw2 = derivatives(#여기)\n","    gradients = np.array([#여기])\n","\n","    params = #여기\n","    history.append(params)\n","\n","print(\"2차원 경사하강법 실행 결과\")\n","print(\"-\" * 30)\n","for i, p in enumerate(history):\n","    if i % 2 == 0:\n","        loss = p[0]**2 + 2*p[1]**2\n","        print(f\"반복 {i}: params = [{p[0]:.4f}, {p[1]:.4f}], 손실 = {loss:.4f}\")\n","\n","print(\"-\" * 30)\n","final_params = history[-1]\n","print(f\"최종 파라미터: [{final_params[0]:.4f}, {final_params[1]:.4f}]\")"],"metadata":{"id":"22AtBQ3x2GD6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----\n","\n","## 과제 1\n","\n","**문제**: 아래에 주어진 손실 함수 `f(x) = x^4 - 3x^3 + 2`의 최솟값을 찾는 경사 하강법 코드를 완성하세요.\n","\n","  * 손실 함수의 도함수(기울기)는 `f'(x) = 4x^3 - 9x^2` 입니다.\n","  * `learning_rate`, `n_iterations`, `initial_x` 값을 자유롭게 조절하여 최솟값(x ≈ 2.25)을 찾아보세요."],"metadata":{"id":"iLh2gE3I2cGx"}},{"cell_type":"code","source":["import numpy as np\n","\n","# 손실 함수: f(x) = x^4 - 3x^3 + 2\n","def loss_function(x):\n","    return x**4 - 3*x**3 + 2\n","\n","# TODO: 아래 함수를 완성하세요.\n","def derivative(x):\n","    #여기\n","    return # 여기에 코드를 작성하세요\n","\n","# TODO: 아래 값들을 조절하여 최적의 x를 찾아보세요.\n","learning_rate = #여기\n","n_iterations = #여기\n","initial_x = #여기\n","\n","\n","x = initial_x\n","print(f\"시작점: x = {x:.4f}, 손실 = {loss_function(x):.4f}\")\n","\n","for i in range(n_iterations):\n","    gradient = derivative(x)\n","    x = #여기\n","\n","print(f\"최종 x 값: {x:.4f}\")\n","print(f\"최종 손실 값: {loss_function(x):.4f}\")"],"metadata":{"id":"TQgJkuMj2ZoM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 과제의 정답 소스"],"metadata":{"id":"IEXrEmF02lOt"}},{"cell_type":"code","source":[],"metadata":{"id":"OYKpORm32ndc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----\n","\n","### \\#\\# 실행 결과\n","\n","```\n","시작점: x = 4.0000, 손실 = 66.0000\n","최종 x 값: 2.2500\n","최종 손실 값: -6.5430\n","```\n","\n","`learning_rate`, `n_iterations`, `initial_x` 값은 위와 달라도 되지만, 최종 `x` 값이 **2.25**에 근접하게 나왔다면 성공적으로 과제를 해결한 것입니다."],"metadata":{"id":"tnnrWeBS2sBQ"}},{"cell_type":"markdown","source":["# 정규 방정식 (OLS)\n","경사하강법 외의 또 다른 해법: 정규 방정식 (OLS)\n","경사 하강법과 같은 반복적인 접근 방식 외에, 최적의 파라미터 β를 한 번의 계산으로 바로 찾아내는 분석적인 방법도 존재합니다. 이를 **정규 방정식(Normal Equation)**이라고 합니다. 이 방법은 행렬 연산, 특히 역행렬 계산을 통해 해를 구하며, 통계학에서는 **최소제곱법(Ordinary Least Squares, OLS)**으로 더 잘 알려져 있습니다.\n","\n","정규 방정식의 공식은 다음과 같이 표현됩니다.\n","$$\\beta = (X^T \\cdot X) ^{-1} \\cdot (X^T \\cdot y) $$\n","\n","이 식은 복잡해 보이지만, 개념적으로는 최적의 파라미터 벡터 β를 구하는 직접적인 공식이라고 이해하면 됩니다.\n","\n","경사 하강법과 정규 방정식은 각각 장단점이 있으며, 둘 사이의 선택은 계산의 복잡성과 알고리즘의 단순성 사이의 근본적인 트레이드오프를 보여줍니다. 정규 방정식은 학습률 같은 하이퍼파라미터를 설정할 필요가 없어 간단해 보이지만, 그 핵심에는 $(X^T X)^{-1}$라는 역행렬 계산이 있습니다. n개의 특성을 가진 데이터에서 n×n 행렬의 역행렬을 계산하는 시간 복잡도는 대략 $O(n^3)$입니다. 이는 특성의 수가 적을 때(예: 10,000개 미만)는 매우 빠르고 효율적이지만, 특성의 수가 수십만 개로 늘어나면 계산이 거의 불가능할 정도로 느려집니다. 반면, 경사 하강법은 반복적인 계산이 필요하지만 특성의 수에 훨씬 더 잘 확장됩니다. 이처럼 알고리즘의 선택은 데이터의 규모에 따라 달라지며, 이는 머신러닝에서 반복적으로 나타나는 중요한 주제입니다. scikit-learn의 LinearRegression 모델은 내부적으로 이 정규 방정식(OLS) 방식을 사용하여 대부분의 일반적인 데이터셋 크기에서 효율적으로 작동합니다."],"metadata":{"id":"J_Oif76aDk-o"}},{"cell_type":"markdown","source":["-----\n","\n","## 과제 2: 시작점에 따라 결과가 달라지는 함수\n","\n","**문제**: 아래에 주어진 손실 함수 `f(x) = x^3 - 6x^2 + 9x + 1`의 \\*\\*지역 최솟값(local minimum)\\*\\*을 찾는 경사 하강법 코드를 완성하세요.\n","\n","이 함수는 시작점(`initial_x`)에 따라 수렴하는 최솟값이 달라질 수 있습니다.\n","\n","  * 손실 함수의 도함수(기울기)는 `f'(x) = 3x^2 - 12x + 9` 입니다.\n","  * **도전 과제**: `initial_x` 값을 `0.0`과 `4.0`으로 각각 설정하여 코드를 두 번 실행해보고, 결과가 어떻게 다른지 확인해보세요."],"metadata":{"id":"k1Ji_GD617_5"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def loss_function(x):\n","    return # 여기\n","\n","# TODO: 아래 함수를 완성하세요.\n","def derivative(x):\n","    return # 여기\n","\n","# TODO: 아래 값들을 조절하여 최적의 x를 찾아보세요.\n","learning_rate = # 여기\n","n_iterations = # 여기\n","initial_x = # 여기에 값을 입력하세요 (0.0 또는 4.0으로 테스트)\n","\n","\n","x = initial_x\n","x_history = [x]\n","\n","print(f\"시작점: x = {x:.4f}, 손실 = {loss_function(x):.4f}\")\n","\n","for i in range(n_iterations):\n","    # 여기\n","\n","\n","\n","print(f\"최종 x 값: {x:.4f}\")\n","print(f\"최종 손실 값: {loss_function(x):.4f}\")\n","\n","# 시각화\n","x_vals = np.linspace(-1, 5, 400)\n","y_vals = loss_function(x_vals)\n","plt.plot(x_vals, y_vals, label='f(x) = x^3 - 6x^2 + 9x + 1')\n","plt.plot(np.array(x_history), loss_function(np.array(x_history)), 'ro-', markersize=4, label='Gradient Descent Path')\n","plt.title('Loss Function & Path')\n","plt.xlabel('x')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"koH0BPwY35X4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----\n","\n","## 과제 3: 2차원 함수의 최솟값 찾기\n","\n","**문제**: 예제 8을 응용하여, 두 개의 변수 $x, y$를 갖는 아래 손실 함수의 최솟값을 찾는 경사 하강법 코드를 완성하세요.\n","\n","  * **손실 함수**: $f(x, y) = (x - 2)^2 + 2(y + 3)^2$\n","  * 이 함수의 최솟값은 $x=2, y=-3$일 때 0입니다.\n","\n"],"metadata":{"id":"lxB0GHE16XuC"}},{"cell_type":"code","source":["import numpy as np\n","\n","# 손실 함수\n","def loss_function(x, y):\n","    return (x - 2)**2 + 2 * (y + 3)**2\n","\n","# TODO: 아래 함수를 완성하세요.\n","def derivatives(x, y):\n","    # 위에서 주어진 편미분 공식을 사용하여 dx와 dy를 계산하세요.\n","    dx = # 여기\n","    dy = # 여기\n","    return dx, dy\n","\n","# TODO: 아래 값들을 조절하여 최적의 파라미터를 찾아보세요.\n","learning_rate = # 여기\n","n_iterations = # 여기\n","params = np.array([-5.0, 5.0]) # [x, y] 시작점\n","\n","\n","history = [params]\n","print(f\"시작점: params = [{params[0]:.4f}, {params[1]:.4f}], 손실 = {loss_function(params[0], params[1]):.4f}\")\n","\n","\n","for i in range(n_iterations):\n","    dx, dy = derivatives(params[0], params[1])\n","    gradients = np.array([# 여기])\n","\n","    params = # 여기\n","    history.append(params)\n","\n","final_params = history[-1]\n","print(f\"최종 파라미터: [{final_params[0]:.4f}, {final_params[1]:.4f}]\")\n","print(f\"최종 손실 값: {loss_function(final_params[0], final_params[1]):.4f}\")"],"metadata":{"id":"RFA0hTPR484w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hXfuhiWO8Nr0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1xyJKDGuGxs2tD9rHnIt-tVhL25tzr6rD","timestamp":1757932624763}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}