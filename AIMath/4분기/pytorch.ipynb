{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-S-XxgE8knF"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.FloatTensor([0., 1., 2., 4., 5., 6.])\n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy87AWoEAjrj",
        "outputId": "615d6dbd-2d1c-4c06-92c7-643c5ca7f108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 2., 4., 5., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(t[0], t[1], t[-1])\n",
        "print(2*t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie2ed--iA7yf",
        "outputId": "d35fd9b9-262b-43a8-891f-3d2e0d71f007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.) tensor(1.) tensor(6.)\n",
            "tensor([ 0.,  2.,  4.,  8., 10., 12.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.FloatTensor([[1., 2., 3.],\n",
        "                       [4., 5., 6.],\n",
        "                       [7., 8., 9.],\n",
        "                       [10., 11., 12.]\n",
        "                       ])"
      ],
      "metadata": {
        "id": "8Bwimq34Bn5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(t[:, 1])\n",
        "print(t[:, 1].size())\n",
        "\n",
        "print(t[:, :-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJRgfpqSCTLC",
        "outputId": "77cd15d7-79ac-47bf-cf54-3278a74a839d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  5.,  8., 11.])\n",
            "torch.Size([4])\n",
            "tensor([[ 1.,  2.],\n",
            "        [ 4.,  5.],\n",
            "        [ 7.,  8.],\n",
            "        [10., 11.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 브로드캐스트 기능 (맞춰주는 기능)\n",
        "m1 = torch.FloatTensor([[1, 2]])\n",
        "m2 = torch.FloatTensor([[3]])\n",
        "\n",
        "print(m1 + m2)\n",
        "\n",
        "m1 = torch.FloatTensor([[1, 2]])\n",
        "m2 = torch.FloatTensor([[3], [4]])\n",
        "\n",
        "print(m1 + m2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHlGgLR5DZ8l",
        "outputId": "0c3da53c-5235-4b99-985d-75de04aba9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 5.]])\n",
            "tensor([[4., 5.],\n",
            "        [5., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "m2 = torch.FloatTensor([[1], [2]])\n",
        "\n",
        "print(m1.matmul(m2))\n",
        "print(m1 @ m2)\n",
        "print(\"--------------------\")\n",
        "print(m1.mul(m2))\n",
        "print(m1 * m2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGeGF0trEr4r",
        "outputId": "4231f658-a414-4793-843e-3e0e62ebc543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5.],\n",
            "        [11.]])\n",
            "tensor([[ 5.],\n",
            "        [11.]])\n",
            "--------------------\n",
            "tensor([[1., 2.],\n",
            "        [6., 8.]])\n",
            "tensor([[1., 2.],\n",
            "        [6., 8.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "print(t.mean())      # 전체 값 평균\n",
        "print(t.mean(dim=0))    # 열 기준으로 평균 계산 (1차원 제거)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymys2hwGFdj9",
        "outputId": "ce81c146-46b6-430c-a8eb-ec67d38646ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.5000)\n",
            "tensor([2., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "print(t.sum())\n",
        "print(t.sum(dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AIoiJHHGLNQ",
        "outputId": "a0695e5d-0a3a-43d7-d621-373b1ff3f908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.)\n",
            "tensor([4., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(t.max(dim=0))   # 뒤에는 인덱스 정보"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM6ITSVLG6qY",
        "outputId": "643b5f7b-6efa-4878-9158-716f88bd6453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([3., 4.]),\n",
            "indices=tensor([1, 1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "t = np.array([[[0, 1, 2],\n",
        "               [3, 4, 5]],\n",
        "              [[6, 7, 8],\n",
        "               [9, 10, 11]]])\n",
        "ft = torch.FloatTensor(t)"
      ],
      "metadata": {
        "id": "nSQXlDVlHjC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ft.view([4, 3]))\n",
        "\n",
        "print(ft.view([4, 1, 3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzFn7BpVIkdm",
        "outputId": "d5aa3651-8909-4ece-9afc-82caa692654b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.],\n",
            "        [ 3.,  4.,  5.],\n",
            "        [ 6.,  7.,  8.],\n",
            "        [ 9., 10., 11.]])\n",
            "tensor([[[ 0.,  1.,  2.]],\n",
            "\n",
            "        [[ 3.,  4.,  5.]],\n",
            "\n",
            "        [[ 6.,  7.,  8.]],\n",
            "\n",
            "        [[ 9., 10., 11.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# squeeze\n",
        "ft = torch.FloatTensor([[0], [1], [2]])\n",
        "print(ft)\n",
        "\n",
        "print(ft.squeeze())\n",
        "\n",
        "ft = torch.FloatTensor([[[0]], [[1]], [[2]]])\n",
        "print(ft.squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlxaLgj5J5-1",
        "outputId": "46d4a1db-04e7-4c6e-dd83-35f2fae133fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [2.]])\n",
            "tensor([0., 1., 2.])\n",
            "tensor([0., 1., 2.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unsqueeze\n",
        "ft = torch.Tensor([0, 1, 2])\n",
        "print(ft.shape)\n",
        "print(ft.unsqueeze(0))\n",
        "print(ft.unsqueeze(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyk381hCKqnL",
        "outputId": "ba36e2fb-1928-4014-abab-e638a161f91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3])\n",
            "tensor([[0., 1., 2.]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [2.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>2번째 pytorch</h1>"
      ],
      "metadata": {
        "id": "v5GTeQuZAHIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t = torch.FloatTensor([1, 1])\n",
        "t.float()"
      ],
      "metadata": {
        "id": "mgNn6z1iLw3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efd0f3f-b41f-4d9f-975c-7c2db310f553"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "y = torch.FloatTensor([[5, 6], [7, 8]])"
      ],
      "metadata": {
        "id": "J2RP2LQVAiYT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat([x,y], dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWHJ-UHsAnUu",
        "outputId": "61ee2408-a692-4172-fa96-7516f900f183"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.],\n",
              "        [5., 6.],\n",
              "        [7., 8.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat([x,y], dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHufyIScA3nb",
        "outputId": "6d46743f-ae2a-4bbf-a145-cc364c69c9bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 5., 6.],\n",
              "        [3., 4., 7., 8.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack([x, y])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxzWJd-NA5FQ",
        "outputId": "6e759fe1-81d5-4385-b9ef-178f482ca1e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 2.],\n",
              "         [3., 4.]],\n",
              "\n",
              "        [[5., 6.],\n",
              "         [7., 8.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor([[1, 2], [3, 4], [5, 6]])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW0um8SiBB8o",
        "outputId": "a72a4e71-3b95-4be2-bc29-bcc97e322b66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.],\n",
              "        [5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros_like(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPNYM4b0Bx5e",
        "outputId": "4abd3abf-1a78-49f5-d574-895ab32a7a0c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>직접 선형회귀 구현하기</h1>"
      ],
      "metadata": {
        "id": "s8D88viqFI1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 데이터\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\n",
        "# 모델 초기화\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정\n",
        "optimizer = torch.optim.SGD([W, b], lr=0.01)\n",
        "\n",
        "nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = x_train * W + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
        "        ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXHLa-V6B1bh",
        "outputId": "c8d343c4-631e-479f-9cb4-86bedbccf1e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1999 W: 0.187, b: 0.080 Cost: 18.666666\n",
            "Epoch  100/1999 W: 1.746, b: 0.578 Cost: 0.048171\n",
            "Epoch  200/1999 W: 1.800, b: 0.454 Cost: 0.029767\n",
            "Epoch  300/1999 W: 1.843, b: 0.357 Cost: 0.018394\n",
            "Epoch  400/1999 W: 1.876, b: 0.281 Cost: 0.011366\n",
            "Epoch  500/1999 W: 1.903, b: 0.221 Cost: 0.007024\n",
            "Epoch  600/1999 W: 1.924, b: 0.174 Cost: 0.004340\n",
            "Epoch  700/1999 W: 1.940, b: 0.136 Cost: 0.002682\n",
            "Epoch  800/1999 W: 1.953, b: 0.107 Cost: 0.001657\n",
            "Epoch  900/1999 W: 1.963, b: 0.084 Cost: 0.001024\n",
            "Epoch 1000/1999 W: 1.971, b: 0.066 Cost: 0.000633\n",
            "Epoch 1100/1999 W: 1.977, b: 0.052 Cost: 0.000391\n",
            "Epoch 1200/1999 W: 1.982, b: 0.041 Cost: 0.000242\n",
            "Epoch 1300/1999 W: 1.986, b: 0.032 Cost: 0.000149\n",
            "Epoch 1400/1999 W: 1.989, b: 0.025 Cost: 0.000092\n",
            "Epoch 1500/1999 W: 1.991, b: 0.020 Cost: 0.000057\n",
            "Epoch 1600/1999 W: 1.993, b: 0.016 Cost: 0.000035\n",
            "Epoch 1700/1999 W: 1.995, b: 0.012 Cost: 0.000022\n",
            "Epoch 1800/1999 W: 1.996, b: 0.010 Cost: 0.000013\n",
            "Epoch 1900/1999 W: 1.997, b: 0.008 Cost: 0.000008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[1], [4], [9]])\n",
        "# 모델 초기화\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정\n",
        "optimizer = torch.optim.SGD([W, b], lr=0.01)\n",
        "\n",
        "nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = x_train ** W + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
        "        ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU8X3G8iFvLj",
        "outputId": "903deecb-c8bf-462f-f536-680331882395"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1999 W: 0.072, b: 0.073 Cost: 24.333334\n",
            "Epoch  100/1999 W: 1.955, b: 0.358 Cost: 0.064149\n",
            "Epoch  200/1999 W: 1.983, b: 0.135 Cost: 0.009166\n",
            "Epoch  300/1999 W: 1.994, b: 0.051 Cost: 0.001302\n",
            "Epoch  400/1999 W: 1.998, b: 0.019 Cost: 0.000185\n",
            "Epoch  500/1999 W: 1.999, b: 0.007 Cost: 0.000026\n",
            "Epoch  600/1999 W: 2.000, b: 0.003 Cost: 0.000004\n",
            "Epoch  700/1999 W: 2.000, b: 0.001 Cost: 0.000001\n",
            "Epoch  800/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch  900/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1000/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1100/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1200/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1300/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1400/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1500/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1600/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1700/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1800/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
            "Epoch 1900/1999 W: 2.000, b: 0.000 Cost: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "y = w**2\n",
        "z = 2*y + 5\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ETxxI_HGAzw",
        "outputId": "b5e45ebb-1932-4254-f2ae-0bf18bfabc9e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# 훈련 데이터\n",
        "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
        "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
        "x3_train = torch.FloatTensor([[75], [93], [80], [100], [70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "# 가중치 w와 편향 b 초기화\n",
        "w1 = torch.zeros(1, requires_grad=True)\n",
        "w2 = torch.zeros(1, requires_grad=True)\n",
        "w3 = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
        "        ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyf8lTavIiDc",
        "outputId": "edc16e1e-8d34-4e26-c318-751c2740dd81"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/10000 w1: 0.294 w2: 0.294 w3: 0.290 b: 0.003 Cost: 29661.800781\n",
            "Epoch  100/10000 w1: 0.688 w2: 0.681 w3: 0.658 b: 0.008 Cost: 5.754573\n",
            "Epoch  200/10000 w1: 0.697 w2: 0.684 w3: 0.645 b: 0.008 Cost: 5.512386\n",
            "Epoch  300/10000 w1: 0.707 w2: 0.686 w3: 0.634 b: 0.008 Cost: 5.281667\n",
            "Epoch  400/10000 w1: 0.715 w2: 0.689 w3: 0.622 b: 0.008 Cost: 5.061868\n",
            "Epoch  500/10000 w1: 0.724 w2: 0.691 w3: 0.611 b: 0.008 Cost: 4.852424\n",
            "Epoch  600/10000 w1: 0.733 w2: 0.693 w3: 0.600 b: 0.008 Cost: 4.652705\n",
            "Epoch  700/10000 w1: 0.741 w2: 0.695 w3: 0.589 b: 0.009 Cost: 4.462287\n",
            "Epoch  800/10000 w1: 0.750 w2: 0.697 w3: 0.579 b: 0.009 Cost: 4.280604\n",
            "Epoch  900/10000 w1: 0.758 w2: 0.699 w3: 0.569 b: 0.009 Cost: 4.107294\n",
            "Epoch 1000/10000 w1: 0.766 w2: 0.700 w3: 0.559 b: 0.009 Cost: 3.941866\n",
            "Epoch 1100/10000 w1: 0.774 w2: 0.702 w3: 0.549 b: 0.009 Cost: 3.783911\n",
            "Epoch 1200/10000 w1: 0.782 w2: 0.703 w3: 0.540 b: 0.009 Cost: 3.633077\n",
            "Epoch 1300/10000 w1: 0.790 w2: 0.704 w3: 0.531 b: 0.009 Cost: 3.488997\n",
            "Epoch 1400/10000 w1: 0.797 w2: 0.706 w3: 0.522 b: 0.009 Cost: 3.351316\n",
            "Epoch 1500/10000 w1: 0.805 w2: 0.707 w3: 0.513 b: 0.009 Cost: 3.219756\n",
            "Epoch 1600/10000 w1: 0.812 w2: 0.708 w3: 0.505 b: 0.009 Cost: 3.093989\n",
            "Epoch 1700/10000 w1: 0.820 w2: 0.709 w3: 0.497 b: 0.009 Cost: 2.973685\n",
            "Epoch 1800/10000 w1: 0.827 w2: 0.709 w3: 0.489 b: 0.009 Cost: 2.858677\n",
            "Epoch 1900/10000 w1: 0.834 w2: 0.710 w3: 0.481 b: 0.009 Cost: 2.748643\n",
            "Epoch 2000/10000 w1: 0.840 w2: 0.711 w3: 0.473 b: 0.009 Cost: 2.643327\n",
            "Epoch 2100/10000 w1: 0.847 w2: 0.711 w3: 0.466 b: 0.009 Cost: 2.542571\n",
            "Epoch 2200/10000 w1: 0.854 w2: 0.712 w3: 0.459 b: 0.009 Cost: 2.446092\n",
            "Epoch 2300/10000 w1: 0.860 w2: 0.712 w3: 0.452 b: 0.009 Cost: 2.353728\n",
            "Epoch 2400/10000 w1: 0.867 w2: 0.712 w3: 0.445 b: 0.009 Cost: 2.265254\n",
            "Epoch 2500/10000 w1: 0.873 w2: 0.713 w3: 0.438 b: 0.009 Cost: 2.180541\n",
            "Epoch 2600/10000 w1: 0.879 w2: 0.713 w3: 0.432 b: 0.010 Cost: 2.099353\n",
            "Epoch 2700/10000 w1: 0.885 w2: 0.713 w3: 0.425 b: 0.010 Cost: 2.021556\n",
            "Epoch 2800/10000 w1: 0.891 w2: 0.713 w3: 0.419 b: 0.010 Cost: 1.946985\n",
            "Epoch 2900/10000 w1: 0.897 w2: 0.713 w3: 0.413 b: 0.010 Cost: 1.875525\n",
            "Epoch 3000/10000 w1: 0.903 w2: 0.713 w3: 0.407 b: 0.010 Cost: 1.806983\n",
            "Epoch 3100/10000 w1: 0.909 w2: 0.713 w3: 0.401 b: 0.010 Cost: 1.741285\n",
            "Epoch 3200/10000 w1: 0.915 w2: 0.713 w3: 0.396 b: 0.010 Cost: 1.678254\n",
            "Epoch 3300/10000 w1: 0.920 w2: 0.713 w3: 0.390 b: 0.010 Cost: 1.617801\n",
            "Epoch 3400/10000 w1: 0.925 w2: 0.713 w3: 0.385 b: 0.010 Cost: 1.559808\n",
            "Epoch 3500/10000 w1: 0.931 w2: 0.713 w3: 0.380 b: 0.010 Cost: 1.504145\n",
            "Epoch 3600/10000 w1: 0.936 w2: 0.713 w3: 0.375 b: 0.010 Cost: 1.450723\n",
            "Epoch 3700/10000 w1: 0.941 w2: 0.712 w3: 0.370 b: 0.010 Cost: 1.399466\n",
            "Epoch 3800/10000 w1: 0.946 w2: 0.712 w3: 0.365 b: 0.010 Cost: 1.350247\n",
            "Epoch 3900/10000 w1: 0.951 w2: 0.712 w3: 0.360 b: 0.010 Cost: 1.303014\n",
            "Epoch 4000/10000 w1: 0.956 w2: 0.712 w3: 0.355 b: 0.010 Cost: 1.257653\n",
            "Epoch 4100/10000 w1: 0.961 w2: 0.711 w3: 0.351 b: 0.010 Cost: 1.214067\n",
            "Epoch 4200/10000 w1: 0.966 w2: 0.711 w3: 0.347 b: 0.010 Cost: 1.172217\n",
            "Epoch 4300/10000 w1: 0.970 w2: 0.711 w3: 0.342 b: 0.010 Cost: 1.132013\n",
            "Epoch 4400/10000 w1: 0.975 w2: 0.710 w3: 0.338 b: 0.010 Cost: 1.093416\n",
            "Epoch 4500/10000 w1: 0.979 w2: 0.710 w3: 0.334 b: 0.010 Cost: 1.056309\n",
            "Epoch 4600/10000 w1: 0.984 w2: 0.709 w3: 0.330 b: 0.010 Cost: 1.020673\n",
            "Epoch 4700/10000 w1: 0.988 w2: 0.709 w3: 0.326 b: 0.010 Cost: 0.986417\n",
            "Epoch 4800/10000 w1: 0.992 w2: 0.708 w3: 0.322 b: 0.010 Cost: 0.953497\n",
            "Epoch 4900/10000 w1: 0.996 w2: 0.708 w3: 0.318 b: 0.010 Cost: 0.921838\n",
            "Epoch 5000/10000 w1: 1.000 w2: 0.707 w3: 0.315 b: 0.010 Cost: 0.891414\n",
            "Epoch 5100/10000 w1: 1.004 w2: 0.707 w3: 0.311 b: 0.010 Cost: 0.862158\n",
            "Epoch 5200/10000 w1: 1.008 w2: 0.706 w3: 0.308 b: 0.010 Cost: 0.834051\n",
            "Epoch 5300/10000 w1: 1.012 w2: 0.706 w3: 0.304 b: 0.010 Cost: 0.807019\n",
            "Epoch 5400/10000 w1: 1.016 w2: 0.705 w3: 0.301 b: 0.010 Cost: 0.781021\n",
            "Epoch 5500/10000 w1: 1.020 w2: 0.705 w3: 0.298 b: 0.010 Cost: 0.756028\n",
            "Epoch 5600/10000 w1: 1.024 w2: 0.704 w3: 0.294 b: 0.010 Cost: 0.731968\n",
            "Epoch 5700/10000 w1: 1.027 w2: 0.704 w3: 0.291 b: 0.010 Cost: 0.708852\n",
            "Epoch 5800/10000 w1: 1.031 w2: 0.703 w3: 0.288 b: 0.010 Cost: 0.686596\n",
            "Epoch 5900/10000 w1: 1.034 w2: 0.703 w3: 0.285 b: 0.010 Cost: 0.665206\n",
            "Epoch 6000/10000 w1: 1.038 w2: 0.702 w3: 0.282 b: 0.010 Cost: 0.644611\n",
            "Epoch 6100/10000 w1: 1.041 w2: 0.702 w3: 0.280 b: 0.010 Cost: 0.624805\n",
            "Epoch 6200/10000 w1: 1.044 w2: 0.701 w3: 0.277 b: 0.010 Cost: 0.605754\n",
            "Epoch 6300/10000 w1: 1.048 w2: 0.700 w3: 0.274 b: 0.010 Cost: 0.587412\n",
            "Epoch 6400/10000 w1: 1.051 w2: 0.700 w3: 0.271 b: 0.010 Cost: 0.569774\n",
            "Epoch 6500/10000 w1: 1.054 w2: 0.699 w3: 0.269 b: 0.010 Cost: 0.552798\n",
            "Epoch 6600/10000 w1: 1.057 w2: 0.699 w3: 0.266 b: 0.010 Cost: 0.536448\n",
            "Epoch 6700/10000 w1: 1.060 w2: 0.698 w3: 0.264 b: 0.009 Cost: 0.520729\n",
            "Epoch 6800/10000 w1: 1.063 w2: 0.697 w3: 0.261 b: 0.009 Cost: 0.505585\n",
            "Epoch 6900/10000 w1: 1.066 w2: 0.697 w3: 0.259 b: 0.009 Cost: 0.491019\n",
            "Epoch 7000/10000 w1: 1.069 w2: 0.696 w3: 0.257 b: 0.009 Cost: 0.476996\n",
            "Epoch 7100/10000 w1: 1.072 w2: 0.696 w3: 0.254 b: 0.009 Cost: 0.463500\n",
            "Epoch 7200/10000 w1: 1.074 w2: 0.695 w3: 0.252 b: 0.009 Cost: 0.450504\n",
            "Epoch 7300/10000 w1: 1.077 w2: 0.695 w3: 0.250 b: 0.009 Cost: 0.437988\n",
            "Epoch 7400/10000 w1: 1.080 w2: 0.694 w3: 0.248 b: 0.009 Cost: 0.425946\n",
            "Epoch 7500/10000 w1: 1.082 w2: 0.694 w3: 0.246 b: 0.009 Cost: 0.414343\n",
            "Epoch 7600/10000 w1: 1.085 w2: 0.693 w3: 0.244 b: 0.009 Cost: 0.403182\n",
            "Epoch 7700/10000 w1: 1.088 w2: 0.692 w3: 0.242 b: 0.009 Cost: 0.392429\n",
            "Epoch 7800/10000 w1: 1.090 w2: 0.692 w3: 0.240 b: 0.009 Cost: 0.382078\n",
            "Epoch 7900/10000 w1: 1.093 w2: 0.691 w3: 0.238 b: 0.009 Cost: 0.372109\n",
            "Epoch 8000/10000 w1: 1.095 w2: 0.691 w3: 0.236 b: 0.009 Cost: 0.362520\n",
            "Epoch 8100/10000 w1: 1.097 w2: 0.690 w3: 0.234 b: 0.009 Cost: 0.353271\n",
            "Epoch 8200/10000 w1: 1.100 w2: 0.690 w3: 0.232 b: 0.009 Cost: 0.344371\n",
            "Epoch 8300/10000 w1: 1.102 w2: 0.689 w3: 0.231 b: 0.009 Cost: 0.335802\n",
            "Epoch 8400/10000 w1: 1.104 w2: 0.689 w3: 0.229 b: 0.009 Cost: 0.327543\n",
            "Epoch 8500/10000 w1: 1.106 w2: 0.688 w3: 0.227 b: 0.009 Cost: 0.319595\n",
            "Epoch 8600/10000 w1: 1.108 w2: 0.688 w3: 0.225 b: 0.009 Cost: 0.311935\n",
            "Epoch 8700/10000 w1: 1.111 w2: 0.687 w3: 0.224 b: 0.009 Cost: 0.304559\n",
            "Epoch 8800/10000 w1: 1.113 w2: 0.686 w3: 0.222 b: 0.009 Cost: 0.297459\n",
            "Epoch 8900/10000 w1: 1.115 w2: 0.686 w3: 0.221 b: 0.009 Cost: 0.290617\n",
            "Epoch 9000/10000 w1: 1.117 w2: 0.685 w3: 0.219 b: 0.009 Cost: 0.284030\n",
            "Epoch 9100/10000 w1: 1.119 w2: 0.685 w3: 0.218 b: 0.009 Cost: 0.277680\n",
            "Epoch 9200/10000 w1: 1.121 w2: 0.684 w3: 0.216 b: 0.009 Cost: 0.271563\n",
            "Epoch 9300/10000 w1: 1.123 w2: 0.684 w3: 0.215 b: 0.009 Cost: 0.265671\n",
            "Epoch 9400/10000 w1: 1.124 w2: 0.683 w3: 0.213 b: 0.009 Cost: 0.260002\n",
            "Epoch 9500/10000 w1: 1.126 w2: 0.683 w3: 0.212 b: 0.008 Cost: 0.254535\n",
            "Epoch 9600/10000 w1: 1.128 w2: 0.683 w3: 0.211 b: 0.008 Cost: 0.249272\n",
            "Epoch 9700/10000 w1: 1.130 w2: 0.682 w3: 0.209 b: 0.008 Cost: 0.244199\n",
            "Epoch 9800/10000 w1: 1.132 w2: 0.682 w3: 0.208 b: 0.008 Cost: 0.239316\n",
            "Epoch 9900/10000 w1: 1.133 w2: 0.681 w3: 0.207 b: 0.008 Cost: 0.234600\n",
            "Epoch 10000/10000 w1: 1.135 w2: 0.681 w3: 0.206 b: 0.008 Cost: 0.230066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "x_train  =  torch.FloatTensor([[73,  80,  75],\n",
        "                               [93,  88,  93],\n",
        "                               [89,  91,  80],\n",
        "                               [96,  98,  100],\n",
        "                               [73,  66,  70]])\n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros((3, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1e-5)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
        "    hypothesis = x_train.matmul(W) + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "      print('Epoch {:4d}/{} W: {} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, W.squeeze().detach(), cost.item()\n",
        "      ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwW3veZTJZjr",
        "outputId": "c3317b98-f422-467d-c73e-3e6a168f2ed1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1000 W: tensor([0.2940, 0.2936, 0.2902]) Cost: 29661.800781\n",
            "Epoch  100/1000 W: tensor([0.6882, 0.6809, 0.6577]) Cost: 5.754573\n",
            "Epoch  200/1000 W: tensor([0.6974, 0.6837, 0.6455]) Cost: 5.512386\n",
            "Epoch  300/1000 W: tensor([0.7065, 0.6863, 0.6336]) Cost: 5.281667\n",
            "Epoch  400/1000 W: tensor([0.7155, 0.6888, 0.6221]) Cost: 5.061907\n",
            "Epoch  500/1000 W: tensor([0.7243, 0.6911, 0.6108]) Cost: 4.852424\n",
            "Epoch  600/1000 W: tensor([0.7330, 0.6932, 0.5999]) Cost: 4.652731\n",
            "Epoch  700/1000 W: tensor([0.7415, 0.6952, 0.5892]) Cost: 4.462265\n",
            "Epoch  800/1000 W: tensor([0.7499, 0.6971, 0.5788]) Cost: 4.280604\n",
            "Epoch  900/1000 W: tensor([0.7581, 0.6988, 0.5687]) Cost: 4.107261\n",
            "Epoch 1000/1000 W: tensor([0.7663, 0.7004, 0.5589]) Cost: 3.941853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEcSo5RjNHK_"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}